{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKndy5rNY5GP",
        "outputId": "b21392e1-e97e-4675-e644-f97734fd643a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.1.2 (from langchain)\n",
            "  Downloading langchain_core-1.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.9.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.4.55)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.12)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain-1.1.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.3-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.1\n",
            "    Uninstalling langchain-core-1.1.1:\n",
            "      Successfully uninstalled langchain-core-1.1.1\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.1.2\n",
            "    Uninstalling langchain-1.1.2:\n",
            "      Successfully uninstalled langchain-1.1.2\n",
            "Successfully installed langchain-1.1.3 langchain-core-1.1.3 langchain-openai-1.1.1\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('SILICONFLOW_API_KEY')"
      ],
      "metadata": {
        "id": "pyqEqFN9ZOMd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æœ¬è¯¾é‡ç‚¹ï¼š**\n",
        "\n",
        "1. **Middleware æ ¸å¿ƒæ¦‚å¿µ** - ç†è§£ Agent æ‰§è¡Œæµç¨‹å’Œé’©å­ç³»ç»Ÿ\n",
        "2. **è£…é¥°å™¨å¼ Middleware** - ä½¿ç”¨ `@before_model`ã€`@after_model`ã€`@wrap_model_call` ç­‰\n",
        "3. **ç±»å¼ Middleware** - ç»§æ‰¿ `AgentMiddleware` åˆ›å»ºå¤æ‚ä¸­é—´ä»¶\n",
        "4. **å†…ç½® Middleware** - ä½¿ç”¨å®˜æ–¹æä¾›çš„ 12+ ä¸ªé¢„ç½®ä¸­é—´ä»¶\n",
        "5. **é«˜çº§æ¨¡å¼** - åŠ¨æ€æç¤ºã€æ¨¡å‹é™çº§ã€äººå·¥å®¡æ ¸ç­‰\n",
        "6. **ç»“æ„åŒ–è¾“å‡º** - ä¸ Pydantic ç»“åˆä½¿ç”¨\n",
        "7. **å®æˆ˜é¡¹ç›®** - æ„å»ºç”Ÿäº§çº§æ™ºèƒ½åŠ©æ‰‹\n",
        "\n",
        "\n",
        "\n",
        "### ä»€ä¹ˆæ˜¯ Middlewareï¼Ÿ\n",
        "\n",
        "Middleware æ˜¯åœ¨ Agent æ‰§è¡Œæµç¨‹ä¸­æ’å…¥çš„é’©å­ï¼ˆhooksï¼‰ï¼Œç”¨äºï¼š\n",
        "\n",
        "- âœ… **æ—¥å¿—ä¸åˆ†æ** - è¿½è¸ªè¡Œä¸ºã€è°ƒè¯•ã€æ€§èƒ½ç›‘æ§\n",
        "- âœ… **è½¬æ¢** - ä¿®æ”¹æç¤ºè¯ã€å·¥å…·é€‰æ‹©ã€è¾“å‡ºæ ¼å¼\n",
        "- âœ… **å®¹é”™** - é‡è¯•ã€é™çº§ã€æ—©æœŸç»ˆæ­¢\n",
        "- âœ… **å®‰å…¨** - é™æµã€å®ˆæŠ¤è§„åˆ™ã€PII æ£€æµ‹\n",
        "\n",
        "\n",
        "\n",
        "Agent æ‰§è¡Œæµç¨‹\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   Agent å¾ªç¯                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                     â”‚\n",
        "â”‚  1. before_model â”€â”€â†’ [å‡†å¤‡é˜¶æ®µ]                     â”‚\n",
        "â”‚         â†“                                           â”‚\n",
        "â”‚  2. modify_model_request â”€â”€â†’ [ä¿®æ”¹è¯·æ±‚]             â”‚\n",
        "â”‚         â†“                                           â”‚\n",
        "â”‚  3. [è°ƒç”¨æ¨¡å‹] â† wrap_model_call                    â”‚\n",
        "â”‚         â†“                                           â”‚\n",
        "â”‚  4. after_model â”€â”€â†’ [å¤„ç†å“åº”]                      â”‚\n",
        "â”‚         â†“                                           â”‚\n",
        "â”‚  5. [å·¥å…·æ‰§è¡Œ] â† wrap_tool_call                     â”‚\n",
        "â”‚         â†“                                           â”‚\n",
        "â”‚  6. [ç»“æŸæˆ–ç»§ç»­å¾ªç¯]                                â”‚\n",
        "â”‚                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### æ ¸å¿ƒé’©å­ï¼ˆHooksï¼‰ä¸€è§ˆ\n",
        "\n",
        "| é’©å­                    | æ—¶æœº         | ç”¨é€”             | å¯å¦ä¿®æ”¹çŠ¶æ€ | å¯å¦è·³è½¬ |\n",
        "| ----------------------- | ------------ | ---------------- | ------------ | -------- |\n",
        "| `@before_agent`         | Agent å¼€å§‹å‰ | åˆå§‹åŒ–ã€æƒé™æ£€æŸ¥ | âœ…            | âœ…        |\n",
        "| `@before_model`         | æ¨¡å‹è°ƒç”¨å‰   | æ—¥å¿—ã€çŠ¶æ€æ›´æ–°   | âœ…            | âœ…        |\n",
        "| `@modify_model_request` | ä¿®æ”¹æ¨¡å‹è¯·æ±‚ | æ”¹æç¤ºè¯ã€å·¥å…·   | âŒ            | âŒ        |\n",
        "| `@wrap_model_call`      | åŒ…è£¹æ¨¡å‹è°ƒç”¨ | é‡è¯•ã€ç¼“å­˜       | âŒ            | âŒ        |\n",
        "| `@after_model`          | æ¨¡å‹å“åº”å   | æ—¥å¿—ã€éªŒè¯       | âœ…            | âœ…        |\n",
        "| `@wrap_tool_call`       | åŒ…è£¹å·¥å…·è°ƒç”¨ | ç›‘æ§ã€é‡è¯•       | âŒ            | âŒ        |\n",
        "| `@dynamic_prompt`       | ç”Ÿæˆç³»ç»Ÿæç¤º | åŠ¨æ€ä¸Šä¸‹æ–‡       | âŒ            | âŒ        |\n",
        "| `@after_agent`          | Agent ç»“æŸå | æ¸…ç†ã€ç»Ÿè®¡       | âœ…            | âŒ        |"
      ],
      "metadata": {
        "id": "jLGygjsYZX9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "æœ€ç®€å•çš„ Middleware ç¤ºä¾‹\n",
        "\"\"\"\n",
        "\n",
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import before_model, after_model, AgentState\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# 1. å®šä¹‰ before_model é’©å­\n",
        "@before_model\n",
        "def log_before(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"åœ¨æ¯æ¬¡è°ƒç”¨æ¨¡å‹å‰æ‰“å°æ—¥å¿—\"\"\"\n",
        "    print(f\"ğŸ“ before_modelå‡†å¤‡è°ƒç”¨æ¨¡å‹ï¼Œå½“å‰æ¶ˆæ¯æ•°ï¼š{len(state['messages'])}\")\n",
        "    return None  # ä¸ä¿®æ”¹çŠ¶æ€\n",
        "\n",
        "# 2. å®šä¹‰ after_model é’©å­\n",
        "@after_model\n",
        "def log_after(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"åœ¨æ¯æ¬¡æ¨¡å‹å“åº”åæ‰“å°æ—¥å¿—\"\"\"\n",
        "    last_message = state['messages'][-1]\n",
        "    print(f\"âœ… after_modelæ¨¡å‹å“åº”ï¼š{last_message.content[:50]}...\")\n",
        "    return None\n",
        "\n",
        "# 3. åˆ›å»ºæ¨¡å‹\n",
        "model = init_chat_model(\n",
        "    \"deepseek-chat\", # Changed from \"deep-seek\" to \"deepseek-chat\"\n",
        "    model_provider=\"openai\",\n",
        "    base_url=\"https://api.deepseek.com\",\n",
        "    api_key= API_KEY\n",
        ")\n",
        "\n",
        "# 4. åˆ›å»º Agentï¼ˆæ·»åŠ  middlewareï¼‰\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[],\n",
        "    middleware=[log_before, log_after]  # ğŸ‘ˆ æ·»åŠ ä¸­é—´ä»¶\n",
        ")\n",
        "\n",
        "# 5. æµ‹è¯•\n",
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"ä½ å¥½ï¼\"}]})\n",
        "print(result)\n",
        "print(f\"\\næœ€ç»ˆç»“æœï¼š{result['messages'][-1].content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "augrmpkLZvWW",
        "outputId": "85bd040a-be40-41da-f8e8-c5d19cfa5deb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ before_modelå‡†å¤‡è°ƒç”¨æ¨¡å‹ï¼Œå½“å‰æ¶ˆæ¯æ•°ï¼š1\n",
            "âœ… after_modelæ¨¡å‹å“åº”ï¼šä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€éœ€è¦ä»€...\n",
            "{'messages': [HumanMessage(content='ä½ å¥½ï¼', additional_kwargs={}, response_metadata={}, id='86ab7aa8-63fb-4882-ac2f-c7eaf91b3b56'), AIMessage(content='ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ–è€…åªæ˜¯æƒ³èŠèŠå¤©ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›æ”¯æŒï¼\\n\\næˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼ŒååŠ©å¤„ç†æ–‡æ¡£ï¼Œè¿›è¡Œåˆ›ä½œå’Œåˆ†æç­‰ç­‰ã€‚è€Œä¸”æˆ‘æ˜¯å®Œå…¨å…è´¹çš„å“¦ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å—ï¼Ÿ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 6, 'total_tokens': 76, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 6}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': '14d0f9e8-e680-405d-9338-f0e79e6adecd', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b083d-b3ab-7983-bfcf-8388df135492-0', usage_metadata={'input_tokens': 6, 'output_tokens': 70, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}\n",
            "\n",
            "æœ€ç»ˆç»“æœï¼šä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ–è€…åªæ˜¯æƒ³èŠèŠå¤©ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›æ”¯æŒï¼\n",
            "\n",
            "æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼ŒååŠ©å¤„ç†æ–‡æ¡£ï¼Œè¿›è¡Œåˆ›ä½œå’Œåˆ†æç­‰ç­‰ã€‚è€Œä¸”æˆ‘æ˜¯å®Œå…¨å…è´¹çš„å“¦ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å—ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `@before_model` å’Œ `@after_model`\n",
        "\n",
        "**ç­¾åï¼š**\n",
        "\n",
        "```\n",
        "def before_model(\n",
        "    state: AgentState,\n",
        "    runtime: Runtime\n",
        ") -> dict[str, Any] | None:\n",
        "    \"\"\"\n",
        "    è¿”å›å€¼ï¼š\n",
        "    - None: ä¸ä¿®æ”¹çŠ¶æ€\n",
        "    - dict: æ›´æ–°çŠ¶æ€ï¼Œå¯åŒ…å« \"jump_to\" é”®è·³è½¬èŠ‚ç‚¹\n",
        "    \"\"\"\n",
        "    \n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-jKV6_ZWjmEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **ç¤ºä¾‹ 1ï¼šæ¶ˆæ¯æ•°é‡é™åˆ¶**\n",
        "\n",
        "from langchain.agents.middleware import before_model, AgentState\n",
        "from langchain.messages import AIMessage\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "\n",
        "@before_model(can_jump_to=[\"end\"])  # ğŸ‘ˆ å£°æ˜å¯ä»¥è·³è½¬åˆ° \"end\"\n",
        "def check_message_limit(\n",
        "    state: AgentState,\n",
        "    runtime: Runtime\n",
        ") -> dict[str, Any] | None:\n",
        "    \"\"\"é™åˆ¶å¯¹è¯è½®æ¬¡ï¼Œè¶…è¿‡ 50 è½®è‡ªåŠ¨ç»“æŸ\"\"\"\n",
        "    if len(state[\"messages\"]) >= 50:\n",
        "        print(\"âš ï¸  å¯¹è¯å·²è¾¾ä¸Šé™ï¼Œè‡ªåŠ¨ç»“æŸ\")\n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=\"å¯¹è¯å·²è¾¾ä¸Šé™ï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼\")],\n",
        "            \"jump_to\": \"end\"  # ğŸ‘ˆ è·³è½¬åˆ°ç»“æŸèŠ‚ç‚¹\n",
        "        }\n",
        "    return None\n",
        "\n",
        "\n",
        "# **ç¤ºä¾‹ 2ï¼šç»Ÿè®¡ Token ä½¿ç”¨**\n",
        "from langchain.agents.middleware import after_model, AgentState\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "\n",
        "class TokenCounter:\n",
        "    \"\"\"Token è®¡æ•°å™¨\"\"\"\n",
        "    def __init__(self):\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def create_hook(self):\n",
        "        \"\"\"åˆ›å»ºé’©å­é—­åŒ…\"\"\"\n",
        "        @after_model\n",
        "        def count_tokens(\n",
        "            state: AgentState,\n",
        "            runtime: Runtime\n",
        "        ) -> dict[str, Any] | None:\n",
        "            # ä»æ¨¡å‹å“åº”ä¸­æå– token ä½¿ç”¨æƒ…å†µ\n",
        "            last_message = state[\"messages\"][-1]\n",
        "            if hasattr(last_message, \"response_metadata\"):\n",
        "                usage = last_message.response_metadata.get(\"usage\", {})\n",
        "                tokens = usage.get(\"total_tokens\", 0)\n",
        "                self.total_tokens += tokens\n",
        "                print(f\"ğŸ“Š æœ¬æ¬¡ä½¿ç”¨ {tokens} tokensï¼Œç´¯è®¡ {self.total_tokens}\")\n",
        "            return None\n",
        "\n",
        "        return count_tokens\n",
        "\n",
        "# ä½¿ç”¨\n",
        "counter = TokenCounter()\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    middleware=[counter.create_hook()]\n",
        ")"
      ],
      "metadata": {
        "id": "zNK7P-X1jQiy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `@wrap_model_call` - åŒ…è£¹æ¨¡å‹è°ƒç”¨\n",
        "ç”¨äºæ‹¦æˆªã€é‡è¯•ã€ç¼“å­˜æ¨¡å‹è°ƒç”¨ã€‚\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def wrap_model_call(\n",
        "    request: ModelRequest,\n",
        "    handler: Callable[[ModelRequest], ModelResponse]\n",
        ") -> ModelResponse:\n",
        "    \"\"\"\n",
        "    request: åŒ…å« model, messages, system_message, tools, state\n",
        "    handler: æ‰§è¡Œå®é™…æ¨¡å‹è°ƒç”¨çš„å‡½æ•°\n",
        "    è¿”å›ï¼šModelResponse\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LB2G1AnokBvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¤ºä¾‹ 1ï¼šé‡è¯•é€»è¾‘**"
      ],
      "metadata": {
        "id": "-D3lFP-nmL1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from typing import Callable\n",
        "import time\n",
        "\n",
        "@wrap_model_call\n",
        "def retry_model(\n",
        "    request: ModelRequest,\n",
        "    handler: Callable[[ModelRequest], ModelResponse]\n",
        ") -> ModelResponse:\n",
        "    \"\"\"è‡ªåŠ¨é‡è¯•å¤±è´¥çš„æ¨¡å‹è°ƒç”¨\"\"\"\n",
        "    max_retries = 3\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"ğŸ”„ å°è¯•è°ƒç”¨æ¨¡å‹ï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼‰\")\n",
        "            return handler(request)\n",
        "        except Exception as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"âŒ æ‰€æœ‰é‡è¯•å¤±è´¥ï¼š{e}\")\n",
        "                raise\n",
        "\n",
        "            # æŒ‡æ•°é€€é¿\n",
        "            wait_time = 2 ** attempt\n",
        "            print(f\"âš ï¸  è°ƒç”¨å¤±è´¥ï¼š{e}ï¼Œ{wait_time} ç§’åé‡è¯•\")\n",
        "            time.sleep(wait_time)"
      ],
      "metadata": {
        "id": "QxekpwD9kLHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¤ºä¾‹ 2ï¼šå“åº”ç¼“å­˜**"
      ],
      "metadata": {
        "id": "nMYXFAbPxSvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from typing import Callable\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "class ModelCache:\n",
        "    \"\"\"æ¨¡å‹å“åº”ç¼“å­˜\"\"\"\n",
        "    def __init__(self):\n",
        "        self.cache = {}\n",
        "\n",
        "    def create_hook(self):\n",
        "        @wrap_model_call\n",
        "        def cache_model(\n",
        "            request: ModelRequest,\n",
        "            handler: Callable[[ModelRequest], ModelResponse]\n",
        "        ) -> ModelResponse:\n",
        "            # ç”Ÿæˆç¼“å­˜é”®\n",
        "            cache_key = hashlib.md5(\n",
        "                json.dumps({\n",
        "                    \"messages\": [str(m) for m in request.messages],\n",
        "                    \"system\": str(request.system_message)\n",
        "                }).encode()\n",
        "            ).hexdigest()\n",
        "\n",
        "            # æ£€æŸ¥ç¼“å­˜\n",
        "            if cache_key in self.cache:\n",
        "                print(\"ğŸ’¾ ç¼“å­˜å‘½ä¸­ï¼\")\n",
        "                return self.cache[cache_key]\n",
        "\n",
        "            # è°ƒç”¨æ¨¡å‹\n",
        "            print(\"ğŸ” ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨æ¨¡å‹\")\n",
        "            response = handler(request)\n",
        "\n",
        "            # å­˜å…¥ç¼“å­˜\n",
        "            self.cache[cache_key] = response\n",
        "            return response\n",
        "\n",
        "        return cache_model\n",
        "\n",
        "# ä½¿ç”¨\n",
        "cache = ModelCache()\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    middleware=[cache.create_hook()]\n",
        ")"
      ],
      "metadata": {
        "id": "NPW01DS4xawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¤ºä¾‹ 3ï¼šä¿®æ”¹ç³»ç»Ÿæç¤º**"
      ],
      "metadata": {
        "id": "zSpk1nWTmRLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from langchain_core.messages import SystemMessage\n",
        "from typing import Callable\n",
        "\n",
        "@wrap_model_call\n",
        "def add_context(\n",
        "    request: ModelRequest,\n",
        "    handler: Callable[[ModelRequest], ModelResponse]\n",
        ") -> ModelResponse:\n",
        "    \"\"\"åŠ¨æ€æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°ç³»ç»Ÿæç¤º\"\"\"\n",
        "\n",
        "    # è·å–å½“å‰æ—¶é—´\n",
        "    from datetime import datetime\n",
        "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # æ„å»ºæ–°çš„ç³»ç»Ÿæ¶ˆæ¯\n",
        "    original_content = request.system_message.content if request.system_message else \"\"\n",
        "    new_content = f\"\"\"{original_content}\n",
        "\n",
        "      å½“å‰æ—¶é—´ï¼š{current_time}\n",
        "      ç”¨æˆ·ä½ç½®ï¼šä¸­å›½\n",
        "      è¯­è¨€åå¥½ï¼šä¸­æ–‡\n",
        "      \"\"\"\n",
        "\n",
        "    # åˆ›å»ºæ–°çš„ç³»ç»Ÿæ¶ˆæ¯\n",
        "    new_system_message = SystemMessage(content=new_content)\n",
        "\n",
        "    # ä½¿ç”¨ override æ–¹æ³•ä¿®æ”¹è¯·æ±‚\n",
        "    modified_request = request.override(system_message=new_system_message)\n",
        "\n",
        "    return handler(modified_request)"
      ],
      "metadata": {
        "id": "G02YB-YRmTBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `@wrap_tool_call` - åŒ…è£¹å·¥å…·è°ƒç”¨\n",
        "ç”¨äºç›‘æ§ã€é‡è¯•ã€ä¿®æ”¹å·¥å…·æ‰§è¡Œã€‚"
      ],
      "metadata": {
        "id": "r5BTC5C-m_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_tool_call\n",
        "from langchain.tools.tool_node import ToolCallRequest\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langgraph.types import Command\n",
        "from typing import Callable\n",
        "import time\n",
        "\n",
        "@wrap_tool_call\n",
        "def monitor_tool(\n",
        "    request: ToolCallRequest,\n",
        "    handler: Callable[[ToolCallRequest], ToolMessage | Command]\n",
        ") -> ToolMessage | Command:\n",
        "    \"\"\"ç›‘æ§å·¥å…·æ‰§è¡Œæ—¶é—´å’ŒçŠ¶æ€\"\"\"\n",
        "\n",
        "    tool_name = request.tool_call[\"name\"]\n",
        "    tool_args = request.tool_call.get(\"args\", {})\n",
        "\n",
        "    print(f\"ğŸ”§ å¼€å§‹æ‰§è¡Œå·¥å…·ï¼š{tool_name}\")\n",
        "    print(f\"   å‚æ•°ï¼š{tool_args}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        result = handler(request)\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ï¼š{elapsed:.2f}ç§’\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{e}ï¼Œè€—æ—¶ï¼š{elapsed:.2f}ç§’\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "I5eXgSSrnCvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `@dynamic_prompt` - åŠ¨æ€ç³»ç»Ÿæç¤º\n",
        "\n",
        "æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€ç”Ÿæˆç³»ç»Ÿæç¤ºè¯ã€‚"
      ],
      "metadata": {
        "id": "m-l-aR72nGPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
        "\n",
        "@dynamic_prompt\n",
        "def user_role_prompt(request: ModelRequest) -> str:\n",
        "    \"\"\"æ ¹æ®ç”¨æˆ·è§’è‰²ç”Ÿæˆä¸åŒçš„ç³»ç»Ÿæç¤º\"\"\"\n",
        "\n",
        "    # ä» runtime context è·å–ç”¨æˆ·è§’è‰²\n",
        "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
        "\n",
        "    base_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚\"\n",
        "\n",
        "    if user_role == \"expert\":\n",
        "        return f\"{base_prompt}\\nè¯·æä¾›è¯¦ç»†çš„æŠ€æœ¯è§£é‡Šï¼Œä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€‚\"\n",
        "    elif user_role == \"beginner\":\n",
        "        return f\"{base_prompt}\\nè¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šï¼Œé¿å…ä¸“ä¸šæœ¯è¯­ã€‚\"\n",
        "    elif user_role == \"child\":\n",
        "        return f\"{base_prompt}\\nè¯·ç”¨å„¿ç«¥èƒ½ç†è§£çš„æ–¹å¼è§£é‡Šï¼Œä½¿ç”¨æ¯”å–»å’Œæ•…äº‹ã€‚\"\n",
        "    else:\n",
        "        return base_prompt\n",
        "\n",
        "# ä½¿ç”¨æ—¶ä¼ å…¥ context\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    middleware=[user_role_prompt],\n",
        "    tools=[]\n",
        ")\n",
        "\n",
        "# è°ƒç”¨æ—¶æŒ‡å®šç”¨æˆ·è§’è‰²\n",
        "result = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\"}]},\n",
        "    config={\"configurable\": {\"user_role\": \"child\"}}\n",
        ")"
      ],
      "metadata": {
        "id": "Smca7Y1anKZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# åˆ›å»º(ç±»å¼)è‡ªå®šä¹‰ Middleware\n",
        "\n",
        "å½“éœ€è¦å¤šä¸ªé’©å­æˆ–æœ‰çŠ¶æ€æ—¶ï¼Œä½¿ç”¨ç±»ç»§æ‰¿ `AgentMiddleware`ã€‚"
      ],
      "metadata": {
        "id": "mzHH3PN_n_zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any, Callable\n",
        "\n",
        "class CustomMiddleware(AgentMiddleware):\n",
        "    \"\"\"è‡ªå®šä¹‰ä¸­é—´ä»¶åŸºç¡€æ¨¡æ¿\"\"\"\n",
        "\n",
        "    def __init__(self, config: dict = None):\n",
        "        \"\"\"åˆå§‹åŒ–é…ç½®\"\"\"\n",
        "        self.config = config or {}\n",
        "\n",
        "    def before_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"Agent å¼€å§‹å‰æ‰§è¡Œ\"\"\"\n",
        "        return None\n",
        "\n",
        "    def before_model(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"æ¨¡å‹è°ƒç”¨å‰æ‰§è¡Œ\"\"\"\n",
        "        return None\n",
        "\n",
        "    def modify_model_request(\n",
        "        self,\n",
        "        request: ModelRequest\n",
        "    ) -> ModelRequest:\n",
        "        \"\"\"ä¿®æ”¹æ¨¡å‹è¯·æ±‚\"\"\"\n",
        "        return request\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        \"\"\"åŒ…è£¹æ¨¡å‹è°ƒç”¨\"\"\"\n",
        "        return handler(request)\n",
        "\n",
        "    def after_model(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"æ¨¡å‹å“åº”åæ‰§è¡Œ\"\"\"\n",
        "        return None\n",
        "\n",
        "    def wrap_tool_call(\n",
        "        self,\n",
        "        request: ToolCallRequest,\n",
        "        handler: Callable[[ToolCallRequest], ToolMessage | Command]\n",
        "    ) -> ToolMessage | Command:\n",
        "        \"\"\"åŒ…è£¹å·¥å…·è°ƒç”¨\"\"\"\n",
        "        return handler(request)\n",
        "\n",
        "    def after_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"Agent ç»“æŸåæ‰§è¡Œ\"\"\"\n",
        "        return None"
      ],
      "metadata": {
        "id": "fjDUFlbWoCu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**å®æˆ˜ç¤ºä¾‹ï¼šå®Œæ•´çš„æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§ä¸­é—´ä»¶**"
      ],
      "metadata": {
        "id": "Yj9CaEL2oUQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\n",
        "from langchain.tools.tool_node import ToolCallRequest\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langgraph.types import Command\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any, Callable\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class PerformanceMonitorMiddleware(AgentMiddleware):\n",
        "    \"\"\"æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶\"\"\"\n",
        "\n",
        "    def __init__(self, log_file: str = \"agent_performance.jsonl\"):\n",
        "        self.log_file = log_file\n",
        "        self.session_start = None\n",
        "        self.model_call_count = 0\n",
        "        self.tool_call_count = 0\n",
        "        self.total_model_time = 0.0\n",
        "        self.total_tool_time = 0.0\n",
        "\n",
        "    def before_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"è®°å½•ä¼šè¯å¼€å§‹\"\"\"\n",
        "        self.session_start = time.time()\n",
        "        print(f\"ğŸš€ Agent ä¼šè¯å¼€å§‹ï¼š{datetime.now()}\")\n",
        "        return None\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        \"\"\"ç›‘æ§æ¨¡å‹è°ƒç”¨\"\"\"\n",
        "        start = time.time()\n",
        "        self.model_call_count += 1\n",
        "\n",
        "        try:\n",
        "            response = handler(request)\n",
        "            elapsed = time.time() - start\n",
        "            self.total_model_time += elapsed\n",
        "\n",
        "            print(f\"ğŸ“Š æ¨¡å‹è°ƒç”¨ #{self.model_call_count}ï¼š{elapsed:.2f}ç§’\")\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            elapsed = time.time() - start\n",
        "            print(f\"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰\")\n",
        "            raise\n",
        "\n",
        "    def wrap_tool_call(\n",
        "        self,\n",
        "        request: ToolCallRequest,\n",
        "        handler: Callable[[ToolCallRequest], ToolMessage | Command]\n",
        "    ) -> ToolMessage | Command:\n",
        "        \"\"\"ç›‘æ§å·¥å…·è°ƒç”¨\"\"\"\n",
        "        start = time.time()\n",
        "        self.tool_call_count += 1\n",
        "        tool_name = request.tool_call[\"name\"]\n",
        "\n",
        "        try:\n",
        "            result = handler(request)\n",
        "            elapsed = time.time() - start\n",
        "            self.total_tool_time += elapsed\n",
        "\n",
        "            print(f\"ğŸ”§ å·¥å…·è°ƒç”¨ #{self.tool_call_count} ({tool_name})ï¼š{elapsed:.2f}ç§’\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            elapsed = time.time() - start\n",
        "            print(f\"âŒ å·¥å…·è°ƒç”¨å¤±è´¥ ({tool_name})ï¼š{e}ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰\")\n",
        "            raise\n",
        "\n",
        "    def after_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"è¾“å‡ºæ€§èƒ½ç»Ÿè®¡\"\"\"\n",
        "        total_time = time.time() - self.session_start\n",
        "\n",
        "        stats = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"total_time\": round(total_time, 2),\n",
        "            \"model_calls\": self.model_call_count,\n",
        "            \"tool_calls\": self.tool_call_count,\n",
        "            \"avg_model_time\": round(self.total_model_time / max(self.model_call_count, 1), 2),\n",
        "            \"avg_tool_time\": round(self.total_tool_time / max(self.tool_call_count, 1), 2),\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ“Š æ€§èƒ½ç»Ÿè®¡\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"æ€»è€—æ—¶ï¼š{stats['total_time']}ç§’\")\n",
        "        print(f\"æ¨¡å‹è°ƒç”¨ï¼š{stats['model_calls']} æ¬¡ï¼Œå¹³å‡ {stats['avg_model_time']}ç§’\")\n",
        "        print(f\"å·¥å…·è°ƒç”¨ï¼š{stats['tool_calls']} æ¬¡ï¼Œå¹³å‡ {stats['avg_tool_time']}ç§’\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        # å†™å…¥æ—¥å¿—æ–‡ä»¶\n",
        "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(stats, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "Vj9v_giaoVOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   LangChain å®˜æ–¹å†…ç½® Middleware\n",
        "\n",
        "LangChain 1.0 æä¾›äº† **12+ ä¸ªé¢„ç½®ä¸­é—´ä»¶**ï¼Œå¼€ç®±å³ç”¨ã€‚"
      ],
      "metadata": {
        "id": "-HOrQFB5ol97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1ã€`SummarizationMiddleware` - å¯¹è¯å†å²æ€»ç»“\n",
        "\n",
        "å½“å¯¹è¯å†å²è¶…è¿‡ token é™åˆ¶æ—¶ï¼Œè‡ªåŠ¨æ€»ç»“å†å²æ¶ˆæ¯ã€‚"
      ],
      "metadata": {
        "id": "aOew26-Xop_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import SummarizationMiddleware\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[],\n",
        "    middleware=[\n",
        "        SummarizationMiddleware(\n",
        "            model=\"openai:gpt-4o-mini\",  # ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹æ€»ç»“\n",
        "            max_tokens_before_summary=400,  # è¶…è¿‡ 400 tokens è§¦å‘æ€»ç»“\n",
        "            messages_to_keep=5  # ä¿ç•™æœ€è¿‘ 5 æ¡æ¶ˆæ¯\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "-wm4kVgAos57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2ã€ `HumanInTheLoopMiddleware` - äººå·¥å®¡æ ¸\n",
        "æš‚åœ Agent æ‰§è¡Œï¼Œè®©ç”¨æˆ·å®¡æ ¸ã€ç¼–è¾‘æˆ–æ‹’ç»å·¥å…·è°ƒç”¨ã€‚\n"
      ],
      "metadata": {
        "id": "ZjMlceTIo0DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[web_search, send_email],  # æ•æ„Ÿæ“ä½œ\n",
        "    middleware=[\n",
        "        HumanInTheLoopMiddleware(\n",
        "            # éœ€è¦äººå·¥å®¡æ ¸çš„å·¥å…·\n",
        "            tools_requiring_approval=[\"send_email\"],\n",
        "            # æˆ–è€…æ‰€æœ‰å·¥å…·éƒ½éœ€è¦å®¡æ ¸\n",
        "            # require_approval_for_all_tools=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# æ‰§è¡Œæ—¶ä¼šæš‚åœç­‰å¾…äººå·¥ç¡®è®¤\n",
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"å‘é‚®ä»¶ç»™è€æ¿\"}]})"
      ],
      "metadata": {
        "id": "aSv7ODUko9SB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c45bc02a-5db2-4f84-fdfd-4186df2b6129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'web_search' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3292393400.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m agent = create_agent(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai:gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweb_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend_email\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# æ•æ„Ÿæ“ä½œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     middleware=[\n\u001b[1;32m      8\u001b[0m         HumanInTheLoopMiddleware(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'web_search' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3ã€`ModelCallLimitMiddleware` - æ¨¡å‹è°ƒç”¨é™åˆ¶\n",
        "é˜²æ­¢æ— é™å¾ªç¯æˆ–æ»¥ç”¨ã€‚"
      ],
      "metadata": {
        "id": "3qm9elrHpCmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
        "\n",
        "limiter = ModelCallLimitMiddleware(\n",
        "    thread_limit=20,  # æ¯ä¸ªçº¿ç¨‹æœ€å¤š 20 æ¬¡æ¨¡å‹è°ƒç”¨\n",
        "    run_limit=10,     # æ¯æ¬¡è¿è¡Œæœ€å¤š 10 æ¬¡\n",
        "    exit_behavior=\"continue\"  # è¾¾åˆ°é™åˆ¶åç»§ç»­æ‰§è¡Œï¼ˆä¸æŠ›å¼‚å¸¸ï¼‰\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    middleware=[limiter]\n",
        ")"
      ],
      "metadata": {
        "id": "4syd6yhCpMSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4ã€`ToolCallLimitMiddleware` - å·¥å…·è°ƒç”¨é™åˆ¶"
      ],
      "metadata": {
        "id": "ms1g24CepaJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
        "\n",
        "tool_limiter = ToolCallLimitMiddleware(\n",
        "    thread_limit=50,  # æ¯ä¸ªçº¿ç¨‹æœ€å¤š 50 æ¬¡å·¥å…·è°ƒç”¨\n",
        "    run_limit=20      # æ¯æ¬¡è¿è¡Œæœ€å¤š 20 æ¬¡\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[calculator, web_search],\n",
        "    middleware=[tool_limiter]\n",
        ")"
      ],
      "metadata": {
        "id": "mSyNHMi8pddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5ã€`PIIMiddleware` - æ•æ„Ÿä¿¡æ¯ä¿æŠ¤\n",
        "\n",
        "æ£€æµ‹å¹¶å¤„ç†ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚\n",
        "\n",
        "| ç­–ç•¥ (strategy) | å«ä¹‰ / è¡Œä¸º                                                                                                                                                                                                                 |\n",
        "| ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **`block`**   | å¦‚æœæ£€æµ‹åˆ° PIIï¼Œå°±**æŠ›å‡ºé”™è¯¯**ï¼ˆexception / `PIIDetectionError`ï¼‰ï¼Œ**é˜»æ­¢æ‰§è¡Œ** â€” ä¸å…è®¸ç»§ç»­ã€‚é€‚åˆå¯¹éšç§è¦æ±‚æé«˜ã€ç»ä¸å…è®¸æ³„éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯çš„åœºæ™¯ã€‚([LangChain Reference][1])                                                                                              |\n",
        "| **`redact`**  | å°†æ£€æµ‹åˆ°çš„ PII **æ›¿æ¢ä¸ºå ä½ç¬¦** (placeholder)ï¼Œé€šå¸¸æ˜¯ç±»ä¼¼ `[REDACTED_EMAIL]`ã€`[REDACTED_CREDIT_CARD]` è¿™æ ·çš„æ ‡ç­¾ â€” å³å®Œå…¨â€œæ“¦é™¤/éšè—â€çœŸå®å†…å®¹ã€‚é€‚åˆæ—¥å¿—æ¸…æ´—ã€åˆè§„éœ€æ±‚ã€å…¬å¼€è¾“å‡ºæ—¶éšè—æ•æ„Ÿå†…å®¹ã€‚([LangChain Reference][1])                                                          |\n",
        "| **`mask`**    | å¯¹ PII è¿›è¡Œ**éƒ¨åˆ†é®è”½(masking)**ï¼Œæ¯”å¦‚ä¿¡ç”¨å¡å·å¯èƒ½å˜æˆ `****-****-****-1234`ï¼ˆåªä¿ç•™æœ€åå‡ ä½/éƒ¨åˆ†å¯è§ï¼‰ï¼Œé‚®ç®±å¯èƒ½ä¿ç•™åŸŸåéƒ¨åˆ† + éšè—ç”¨æˆ·åçš„ä¸€éƒ¨åˆ†ç­‰ â€” æ—¢éšè—å¤§éƒ¨åˆ†æ•æ„Ÿä¿¡æ¯ï¼Œåˆä¿ç•™äº†ä¸€ç‚¹â€œå¯è¾¨è¯†æ€§â€ï¼ˆæ¯”å¦‚è´¦å·åå››ä½ã€åŸŸåç­‰ï¼‰ï¼Œé€‚åˆç”¨æˆ·æœåŠ¡ç•Œé¢ / å‰ç«¯æ˜¾ç¤º / éœ€è¦éƒ¨åˆ†å¯è¯†åˆ«ä½†ä¸æ³„éœ²å®Œæ•´æ•æ„Ÿå†…å®¹çš„åœºæ™¯ã€‚([LangChain Reference][1])               |\n",
        "| **`hash`**    | å°†æ£€æµ‹åˆ°çš„ PII è½¬æ¢æˆä¸€ä¸ª**ç¡®å®šæ€§ hash å€¼** (deterministic hash)ï¼Œæ¯”å¦‚ `<email_hash:a1b2c3d4>` è¿™æ ·çš„æ ‡è¯† â€” å¯¹äºåŸå§‹ç”¨æˆ·æ¥è¯´æ˜¯çœŸåŒ¿åï¼ä¸å¯é€†çš„ï¼Œä½†å¯¹ç³»ç»Ÿæ¥è¯´å¯ä»¥â€œåŒºåˆ†/å»é‡/è¿½è¸ªâ€ï¼ˆæ¯”å¦‚ç»Ÿè®¡æŸä¸ªé‚®ç®±æ˜¯å¦å‡ºç°è¿‡å¤šæ¬¡ã€åšè¡Œä¸ºåˆ†æã€æ—¥å¿—å¯¹æ¯”ç­‰ï¼‰ã€‚é€‚åˆ analyticsã€è°ƒè¯• (debug)ã€ç»Ÿè®¡åˆ†æã€åŒ¿åè¿½è¸ªç­‰åœºæ™¯ã€‚([LangChain Reference][1]) |\n",
        "\n",
        "[1]: https://reference.langchain.com/python/langchain/middleware/?utm_source=chatgpt.com \"Middleware | LangChain Reference\"\n"
      ],
      "metadata": {
        "id": "WcPw7MGVpjCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import PIIMiddleware\n",
        "\n",
        "pii_protector = PIIMiddleware(\n",
        "    detection_model=\"openai:gpt-4o-mini\",\n",
        "    strategy=\"redact\",  # ç­–ç•¥ï¼šblock, redact, mask, hash\n",
        "    pii_types=[\"email\", \"phone\", \"ssn\", \"credit_card\"]\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    middleware=[pii_protector],\n",
        "    tools=[]\n",
        ")\n",
        "\n",
        "# è¾“å…¥åŒ…å«æ•æ„Ÿä¿¡æ¯ä¼šè‡ªåŠ¨å¤„ç†\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"æˆ‘çš„é‚®ç®±æ˜¯ user@example.comï¼Œç”µè¯ 13800138000\"\n",
        "    }]\n",
        "})"
      ],
      "metadata": {
        "id": "xNOEkMlGpgKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6ã€`ModelFallbackMiddleware` - æ¨¡å‹é™çº§\n",
        "\n",
        "ä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æ¨¡å‹ã€‚"
      ],
      "metadata": {
        "id": "CsnpnCY0qTMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import ModelFallbackMiddleware\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# å®šä¹‰ä¸»æ¨¡å‹å’Œå¤‡ç”¨æ¨¡å‹\n",
        "primary_model = init_chat_model(\"openai:gpt-4o\")\n",
        "fallback_models = [\n",
        "    init_chat_model(\"openai:gpt-4o-mini\"),\n",
        "    init_chat_model(\"anthropic:claude-3-haiku\")\n",
        "]\n",
        "\n",
        "fallback = ModelFallbackMiddleware(\n",
        "    fallback_models=fallback_models\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=primary_model,\n",
        "    middleware=[fallback],\n",
        "    tools=[]\n",
        ")"
      ],
      "metadata": {
        "id": "N9nVD9W5qWMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7ã€`ToolRetryMiddleware` - å·¥å…·é‡è¯•\n",
        "\n",
        "è‡ªåŠ¨é‡è¯•å¤±è´¥çš„å·¥å…·è°ƒç”¨ã€‚\n",
        "\n",
        "ç¬¬ 1 æ¬¡é‡è¯•ï¼ˆretry_number=1ï¼‰: ç­‰å¾… ~ 1.0 * (2.0 ** 1) = 2.0 ç§’\n",
        "\n",
        "ç¬¬ 2 æ¬¡é‡è¯•ï¼ˆretry_number=2ï¼‰: ç­‰å¾… ~ 1.0 * (2.0 ** 2) = 4.0 ç§’\n",
        "\n",
        "ç¬¬ 3 æ¬¡é‡è¯•ï¼ˆretry_number=3ï¼‰: ç­‰å¾… ~ 1.0 * (2.0 ** 3) = 8.0 ç§’\n",
        "\n",
        "ä¹Ÿå°±æ˜¯è¯´ï¼Œç­‰å¾…æ—¶é—´ä»¥æŒ‡æ•°æ–¹å¼å¢é•¿ â€”â€” æ¯å¤±è´¥ä¸€æ¬¡ï¼Œä¸‹æ¬¡å†è¯•ä¹‹å‰ç­‰å¾…æ›´é•¿æ—¶é—´ã€‚\n",
        "\n",
        "å¦‚æœä½ æŠŠ backoff_factor = 0ï¼Œå°±æ„å‘³ç€ä¸ä½¿ç”¨æŒ‡æ•°å¢é•¿ï¼Œé‡è¯•ä¹‹é—´å§‹ç»ˆç”¨å›ºå®šçš„ initial_delayã€‚"
      ],
      "metadata": {
        "id": "fRgG5I6kqgqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import ToolRetryMiddleware\n",
        "\n",
        "retry = ToolRetryMiddleware(\n",
        "    max_retries=3,\n",
        "    backoff_factor=2.0,  # æŒ‡æ•°é€€é¿å› å­\n",
        "    retry_on_exceptions=[ConnectionError, TimeoutError]\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[web_search, api_call],\n",
        "    middleware=[retry]\n",
        ")"
      ],
      "metadata": {
        "id": "amuHsUW0qjK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8ã€`FilesystemFileSearchMiddleware` - æ–‡ä»¶ç³»ç»Ÿæœç´¢\n",
        "\n",
        "ä¸º Agent æ·»åŠ æ–‡ä»¶æœç´¢èƒ½åŠ›ï¼ˆGlob å’Œ Grepï¼‰ã€‚"
      ],
      "metadata": {
        "id": "AdNcJHmxrITU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import FilesystemFileSearchMiddleware\n",
        "\n",
        "file_search = FilesystemFileSearchMiddleware(\n",
        "    root_path=\"/workspace/project\",  # æœç´¢æ ¹ç›®å½•\n",
        "    allowed_extensions=[\".py\", \".js\", \".md\"],  # å…è®¸çš„æ–‡ä»¶ç±»å‹\n",
        "    max_results=50\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[],  # è‡ªåŠ¨æ·»åŠ  Glob å’Œ Grep å·¥å…·\n",
        "    middleware=[file_search]\n",
        ")\n",
        "\n",
        "# Agent ç°åœ¨å¯ä»¥æœç´¢æ–‡ä»¶\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"å¸®æˆ‘æ‰¾åˆ°æ‰€æœ‰åŒ…å« 'API' çš„ Python æ–‡ä»¶\"\n",
        "    }]\n",
        "})"
      ],
      "metadata": {
        "id": "XA6MaA7IrK6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9ã€`LLMToolSelectorMiddleware` - æ™ºèƒ½å·¥å…·ç­›é€‰\n",
        "\n",
        "å½“å·¥å…·å¤ªå¤šæ—¶ï¼Œç”¨ LLM ç­›é€‰æœ€ç›¸å…³çš„å·¥å…·ã€‚"
      ],
      "metadata": {
        "id": "t3I57flArPkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import LLMToolSelectorMiddleware\n",
        "\n",
        "tool_selector = LLMToolSelectorMiddleware(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    max_tools=5  # æœ€å¤šé€‰æ‹© 5 ä¸ªå·¥å…·\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[...100ä¸ªå·¥å…·...],  # å¾ˆå¤šå·¥å…·\n",
        "    middleware=[tool_selector]\n",
        ")"
      ],
      "metadata": {
        "id": "M5tfVW8irRh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç»„åˆå¤šä¸ª Middleware\n",
        "\n",
        "Middleware å¯ä»¥å åŠ ä½¿ç”¨ï¼Œ**æ‰§è¡Œé¡ºåº**ï¼š\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "è¿›å…¥ï¼š[MW1 before] â†’ [MW2 before] â†’ [MW3 before] â†’ æ¨¡å‹è°ƒç”¨\n",
        "è¿”å›ï¼š[MW3 after] â†’ [MW2 after] â†’ [MW1 after]\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4vhNzTfYrhpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import (\n",
        "    SummarizationMiddleware,\n",
        "    ModelCallLimitMiddleware,\n",
        "    PIIMiddleware,\n",
        "    ToolRetryMiddleware\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o\",\n",
        "    tools=[web_search, calculator],\n",
        "    middleware=[\n",
        "        PIIMiddleware(strategy=\"redact\"),           # 1. æœ€å…ˆæ£€æŸ¥ PII\n",
        "        ModelCallLimitMiddleware(run_limit=10),     # 2. é™åˆ¶è°ƒç”¨æ¬¡æ•°\n",
        "        SummarizationMiddleware(max_tokens_before_summary=500),  # 3. æ€»ç»“å†å²\n",
        "        ToolRetryMiddleware(max_retries=3),         # 4. é‡è¯•å·¥å…·\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ocuwz4_Trneu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# é«˜çº§ Middleware æ¨¡å¼\n",
        "\n",
        "###  1ã€è‡ªå®šä¹‰çŠ¶æ€ï¼ˆCustom Stateï¼‰\n",
        "\n",
        "æ‰©å±• `AgentState` æ·»åŠ è‡ªå®šä¹‰å­—æ®µã€‚"
      ],
      "metadata": {
        "id": "4ZGvNFHUr1m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import AgentState, before_model\n",
        "from typing_extensions import NotRequired\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "\n",
        "# 1. å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€\n",
        "class CustomState(AgentState):\n",
        "    \"\"\"æ‰©å±•çŠ¶æ€ï¼Œæ·»åŠ è‡ªå®šä¹‰å­—æ®µ\"\"\"\n",
        "    model_call_count: NotRequired[int]\n",
        "    user_id: NotRequired[str]\n",
        "    user_preferences: NotRequired[dict]\n",
        "\n",
        "# 2. ä½¿ç”¨è‡ªå®šä¹‰çŠ¶æ€\n",
        "@before_model(state_schema=CustomState, can_jump_to=[\"end\"])\n",
        "def check_user_quota(\n",
        "    state: CustomState,\n",
        "    runtime: Runtime\n",
        ") -> dict[str, Any] | None:\n",
        "    \"\"\"æ£€æŸ¥ç”¨æˆ·é…é¢\"\"\"\n",
        "\n",
        "    user_id = state.get(\"user_id\", \"anonymous\")\n",
        "    call_count = state.get(\"model_call_count\", 0)\n",
        "\n",
        "    # å‡è®¾ç”¨æˆ·æœ‰é…é¢é™åˆ¶\n",
        "    user_quota = 100\n",
        "\n",
        "    if call_count >= user_quota:\n",
        "        print(f\"âš ï¸  ç”¨æˆ· {user_id} å·²è¾¾é…é¢ä¸Šé™\")\n",
        "        return {\n",
        "            \"jump_to\": \"end\",\n",
        "            \"messages\": [AIMessage(content=\"æ‚¨çš„é…é¢å·²ç”¨å®Œï¼Œè¯·å‡çº§å¥—é¤ã€‚\")]\n",
        "        }\n",
        "\n",
        "    # æ›´æ–°è®¡æ•°\n",
        "    return {\"model_call_count\": call_count + 1}\n",
        "\n",
        "# 3. åˆ›å»º Agent æ—¶æŒ‡å®šçŠ¶æ€ schema\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    state_schema=CustomState,  # ğŸ‘ˆ æŒ‡å®šè‡ªå®šä¹‰çŠ¶æ€\n",
        "    middleware=[check_user_quota],\n",
        "    tools=[]\n",
        ")\n",
        "\n",
        "# 4. è°ƒç”¨æ—¶ä¼ å…¥è‡ªå®šä¹‰çŠ¶æ€\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}],\n",
        "    \"user_id\": \"user_123\",\n",
        "    \"model_call_count\": 95\n",
        "})"
      ],
      "metadata": {
        "id": "2LP_vlYer6Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2ã€åŠ¨æ€æ¨¡å‹é€‰æ‹©\n",
        "\n",
        "æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ‹©æ¨¡å‹ã€‚"
      ],
      "metadata": {
        "id": "ZrB3OorWsLrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from langchain.chat_models import init_chat_model\n",
        "from typing import Callable\n",
        "import os\n",
        "\n",
        "class DynamicModelSelector:\n",
        "    \"\"\"åŠ¨æ€æ¨¡å‹é€‰æ‹©å™¨\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # åˆå§‹åŒ–ä¸åŒçº§åˆ«çš„æ¨¡å‹\n",
        "        self.simple_model = init_chat_model(\n",
        "            \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "            model_provider=\"openai\",\n",
        "            base_url=\"https://api.siliconflow.cn/v1\",\n",
        "            api_key=os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "        )\n",
        "\n",
        "        self.complex_model = init_chat_model(\n",
        "            \"Qwen/Qwen2.5-72B-Instruct\",\n",
        "            model_provider=\"openai\",\n",
        "            base_url=\"https://api.siliconflow.cn/v1\",\n",
        "            api_key=os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "        )\n",
        "\n",
        "    def _estimate_complexity(self, messages: list) -> str:\n",
        "        \"\"\"ä¼°ç®—æŸ¥è¯¢å¤æ‚åº¦\"\"\"\n",
        "        if not messages:\n",
        "            return \"simple\"\n",
        "\n",
        "        last_message = str(messages[-1])\n",
        "\n",
        "        # ç®€å•è§„åˆ™åˆ¤æ–­\n",
        "        complex_keywords = [\"åˆ†æ\", \"ä¸ºä»€ä¹ˆ\", \"å¦‚ä½•\", \"åŸç†\", \"è¯¦ç»†\", \"å¤æ‚\"]\n",
        "        if any(kw in last_message for kw in complex_keywords):\n",
        "            return \"complex\"\n",
        "\n",
        "        # æ¶ˆæ¯é•¿åº¦åˆ¤æ–­\n",
        "        if len(last_message) > 100:\n",
        "            return \"complex\"\n",
        "\n",
        "        return \"simple\"\n",
        "\n",
        "    def create_hook(self):\n",
        "        \"\"\"åˆ›å»ºåŠ¨æ€é€‰æ‹©é’©å­\"\"\"\n",
        "\n",
        "        @wrap_model_call\n",
        "        def select_model(\n",
        "            request: ModelRequest,\n",
        "            handler: Callable[[ModelRequest], ModelResponse]\n",
        "        ) -> ModelResponse:\n",
        "            # ä¼°ç®—å¤æ‚åº¦\n",
        "            complexity = self._estimate_complexity(request.messages)\n",
        "\n",
        "            # é€‰æ‹©æ¨¡å‹\n",
        "            if complexity == \"complex\":\n",
        "                print(\"ğŸ§  ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆå¤æ‚ä»»åŠ¡ï¼‰\")\n",
        "                selected_model = self.complex_model\n",
        "            else:\n",
        "                print(\"âš¡ ä½¿ç”¨å°æ¨¡å‹ï¼ˆç®€å•ä»»åŠ¡ï¼‰\")\n",
        "                selected_model = self.simple_model\n",
        "\n",
        "            # ä¿®æ”¹è¯·æ±‚çš„æ¨¡å‹\n",
        "            modified_request = request.override(model=selected_model)\n",
        "            return handler(modified_request)\n",
        "\n",
        "        return select_model\n",
        "\n",
        "# ä½¿ç”¨\n",
        "selector = DynamicModelSelector()\n",
        "agent = create_agent(\n",
        "    model=selector.simple_model,  # é»˜è®¤æ¨¡å‹ï¼ˆä¼šè¢«åŠ¨æ€æ›¿æ¢ï¼‰\n",
        "    middleware=[selector.create_hook()],\n",
        "    tools=[]\n",
        ")\n",
        "\n",
        "# æµ‹è¯•\n",
        "queries = [\n",
        "    \"1+1ç­‰äºå¤šå°‘ï¼Ÿ\",  # ç®€å•\n",
        "    \"è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ çš„åå‘ä¼ æ’­åŸç†\"  # å¤æ‚\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"é—®é¢˜ï¼š{query}\")\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "    print(f\"å›ç­”ï¼š{result['messages'][-1].content[:100]}...\")"
      ],
      "metadata": {
        "id": "vuB3r0JesOyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3ã€æˆæœ¬è¿½è¸ªå’Œé¢„ç®—æ§åˆ¶"
      ],
      "metadata": {
        "id": "QrZyYJVDsoAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any, Callable\n",
        "from datetime import datetime\n",
        "\n",
        "class CostTrackingMiddleware(AgentMiddleware):\n",
        "    \"\"\"æˆæœ¬è¿½è¸ªä¸­é—´ä»¶\"\"\"\n",
        "\n",
        "    def __init__(self, daily_budget: float = 10.0):\n",
        "        self.daily_budget = daily_budget  # æ¯æ—¥é¢„ç®—ï¼ˆå…ƒï¼‰\n",
        "        self.daily_cost = 0.0\n",
        "        self.current_date = datetime.now().date()\n",
        "\n",
        "        # æ¨¡å‹ä»·æ ¼ï¼ˆå…ƒ/ç™¾ä¸‡ tokensï¼‰\n",
        "        self.model_prices = {\n",
        "            \"Qwen/Qwen2.5-7B-Instruct\": 0.35,\n",
        "            \"Qwen/Qwen2.5-72B-Instruct\": 3.5,\n",
        "            \"openai:gpt-4o\": 15.0,\n",
        "            \"openai:gpt-4o-mini\": 1.5,\n",
        "        }\n",
        "\n",
        "    def _reset_if_new_day(self):\n",
        "        \"\"\"æ–°çš„ä¸€å¤©é‡ç½®æˆæœ¬\"\"\"\n",
        "        today = datetime.now().date()\n",
        "        if today != self.current_date:\n",
        "            print(f\"ğŸ“… æ–°çš„ä¸€å¤©ï¼Œé‡ç½®æˆæœ¬ç»Ÿè®¡\")\n",
        "            self.daily_cost = 0.0\n",
        "            self.current_date = today\n",
        "\n",
        "    def _estimate_cost(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        input_tokens: int,\n",
        "        output_tokens: int\n",
        "    ) -> float:\n",
        "        \"\"\"ä¼°ç®—æˆæœ¬\"\"\"\n",
        "        price_per_million = self.model_prices.get(model_name, 1.0)\n",
        "        total_tokens = input_tokens + output_tokens\n",
        "        cost = (total_tokens / 1_000_000) * price_per_million\n",
        "        return cost\n",
        "\n",
        "    def before_model(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        \"\"\"æ£€æŸ¥é¢„ç®—\"\"\"\n",
        "        self._reset_if_new_day()\n",
        "\n",
        "        if self.daily_cost >= self.daily_budget:\n",
        "            print(f\"âš ï¸  å·²è¾¾æ¯æ—¥é¢„ç®—ä¸Šé™ Â¥{self.daily_budget}\")\n",
        "            return {\n",
        "                \"jump_to\": \"end\",\n",
        "                \"messages\": [AIMessage(content=\"ä»Šæ—¥é¢„ç®—å·²ç”¨å®Œï¼Œè¯·æ˜å¤©å†è¯•ã€‚\")]\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        \"\"\"è¿½è¸ªæˆæœ¬\"\"\"\n",
        "        response = handler(request)\n",
        "\n",
        "        # æå– token ä½¿ç”¨æƒ…å†µ\n",
        "        if hasattr(response, \"usage_metadata\"):\n",
        "            usage = response.usage_metadata\n",
        "            input_tokens = usage.get(\"input_tokens\", 0)\n",
        "            output_tokens = usage.get(\"output_tokens\", 0)\n",
        "\n",
        "            # è®¡ç®—æˆæœ¬\n",
        "            model_name = str(request.model)\n",
        "            cost = self._estimate_cost(model_name, input_tokens, output_tokens)\n",
        "            self.daily_cost += cost\n",
        "\n",
        "            print(f\"ğŸ’° æœ¬æ¬¡æˆæœ¬ï¼šÂ¥{cost:.4f}ï¼ˆç´¯è®¡ï¼šÂ¥{self.daily_cost:.4f}/Â¥{self.daily_budget}ï¼‰\")\n",
        "\n",
        "        return response\n",
        "\n",
        "# ä½¿ç”¨\n",
        "cost_tracker = CostTrackingMiddleware(daily_budget=5.0)\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    middleware=[cost_tracker],\n",
        "    tools=[]\n",
        ")"
      ],
      "metadata": {
        "id": "KCELegCXssb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4ã€Anthropic Prompt Caching\n",
        "\n",
        "åˆ©ç”¨ Anthropic çš„æç¤ºè¯ç¼“å­˜åŠŸèƒ½ï¼Œå¤§å¹…é™ä½æˆæœ¬ï¼ˆé€‚ç”¨äº Claude æ¨¡å‹ï¼‰ã€‚"
      ],
      "metadata": {
        "id": "IlS2R9Fus0ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from typing import Callable\n",
        "\n",
        "@wrap_model_call\n",
        "def enable_prompt_caching(\n",
        "    request: ModelRequest,\n",
        "    handler: Callable[[ModelRequest], ModelResponse]\n",
        ") -> ModelResponse:\n",
        "    \"\"\"\n",
        "    ä¸º Anthropic æ¨¡å‹å¯ç”¨ Prompt Caching\n",
        "\n",
        "    å·¥ä½œåŸç†ï¼š\n",
        "    1. åœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ ç¼“å­˜æ ‡è®°\n",
        "    2. åç»­ç›¸åŒçš„ç³»ç»Ÿæ¶ˆæ¯ä¼šå¤ç”¨ç¼“å­˜\n",
        "    3. å¯èŠ‚çœé«˜è¾¾ 90% çš„è¾“å…¥ token æˆæœ¬\n",
        "    \"\"\"\n",
        "\n",
        "    # åªå¯¹ Anthropic æ¨¡å‹ç”Ÿæ•ˆ\n",
        "    if isinstance(request.model, ChatAnthropic):\n",
        "        # ä¿®æ”¹ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ç¼“å­˜æ ‡è®°\n",
        "        if request.system_message:\n",
        "            # ä¸ºç³»ç»Ÿæ¶ˆæ¯çš„æœ€åä¸€ä¸ªå†…å®¹å—æ·»åŠ ç¼“å­˜æ ‡è®°\n",
        "            content_blocks = list(request.system_message.content_blocks)\n",
        "            if content_blocks:\n",
        "                content_blocks[-1][\"cache_control\"] = {\"type\": \"ephemeral\"}\n",
        "\n",
        "                new_system_message = request.system_message.copy()\n",
        "                new_system_message.content = content_blocks\n",
        "\n",
        "                modified_request = request.override(system_message=new_system_message)\n",
        "                return handler(modified_request)\n",
        "\n",
        "    return handler(request)\n",
        "\n",
        "# ä½¿ç”¨\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "anthropic_model = ChatAnthropic(\n",
        "    model=\"claude-3-5-sonnet-20241022\",\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
        ")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=anthropic_model,\n",
        "    middleware=[enable_prompt_caching],\n",
        "    tools=[]\n",
        ")"
      ],
      "metadata": {
        "id": "olXEMohgs6IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ç»“æ„åŒ–è¾“å‡ºä¸æ•°æ®éªŒè¯\n",
        "\n",
        "### 1ã€ ä½¿ç”¨ Pydantic å®šä¹‰è¾“å‡ºç»“æ„\n",
        "\n",
        "ç»“åˆ Middleware å’Œç»“æ„åŒ–è¾“å‡ºï¼Œå®ç°å¼ºç±»å‹çš„ Agent å“åº”ã€‚"
      ],
      "metadata": {
        "id": "W2ftyMfGtiv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹\n",
        "\"\"\"\n",
        "\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import after_model, AgentState\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "import os\n",
        "\n",
        "# 1. å®šä¹‰æ•°æ®æ¨¡å‹\n",
        "class Task(BaseModel):\n",
        "    \"\"\"ä»»åŠ¡\"\"\"\n",
        "    title: str = Field(description=\"ä»»åŠ¡æ ‡é¢˜\")\n",
        "    priority: int = Field(description=\"ä¼˜å…ˆçº§ 1-5\", ge=1, le=5)\n",
        "    deadline: Optional[str] = Field(description=\"æˆªæ­¢æ—¥æœŸ YYYY-MM-DD\")\n",
        "    tags: List[str] = Field(description=\"æ ‡ç­¾åˆ—è¡¨\", default_factory=list)\n",
        "\n",
        "    @field_validator('deadline')\n",
        "    @classmethod\n",
        "    def validate_deadline(cls, v):\n",
        "        \"\"\"éªŒè¯æ—¥æœŸæ ¼å¼\"\"\"\n",
        "        if v:\n",
        "            try:\n",
        "                datetime.strptime(v, '%Y-%m-%d')\n",
        "            except ValueError:\n",
        "                raise ValueError('æ—¥æœŸæ ¼å¼å¿…é¡»æ˜¯ YYYY-MM-DD')\n",
        "        return v\n",
        "\n",
        "class ProjectPlan(BaseModel):\n",
        "    \"\"\"é¡¹ç›®è®¡åˆ’\"\"\"\n",
        "    project_name: str = Field(description=\"é¡¹ç›®åç§°\")\n",
        "    description: str = Field(description=\"é¡¹ç›®æè¿°\")\n",
        "    tasks: List[Task] = Field(description=\"ä»»åŠ¡åˆ—è¡¨\")\n",
        "    total_days: int = Field(description=\"é¢„è®¡æ€»å¤©æ•°\", gt=0)\n",
        "\n",
        "    @field_validator('tasks')\n",
        "    @classmethod\n",
        "    def validate_tasks(cls, v):\n",
        "        \"\"\"éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªä»»åŠ¡\"\"\"\n",
        "        if len(v) == 0:\n",
        "            raise ValueError('è‡³å°‘éœ€è¦ä¸€ä¸ªä»»åŠ¡')\n",
        "        return v\n",
        "\n",
        "# 2. åˆ›å»ºæ¨¡å‹\n",
        "model = init_chat_model(\n",
        "    \"Qwen/Qwen3-8B\",\n",
        "    model_provider=\"openai\",\n",
        "    base_url=\"https://api.siliconflow.cn/v1\",\n",
        "    api_key= API_KEY\n",
        ")\n",
        "\n",
        "# 3. ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º\n",
        "structured_model = model.with_structured_output(ProjectPlan)\n",
        "\n",
        "# 4. æ·»åŠ éªŒè¯ä¸­é—´ä»¶\n",
        "@after_model\n",
        "def validate_output(\n",
        "    state: AgentState,\n",
        "    runtime: Runtime\n",
        ") -> dict[str, Any] | None:\n",
        "    \"\"\"éªŒè¯è¾“å‡ºæ ¼å¼\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    print(f\"âœ… éªŒè¯é€šè¿‡ï¼šç”Ÿæˆäº† {len(last_message.content.tasks)} ä¸ªä»»åŠ¡\")\n",
        "    return None\n",
        "\n",
        "# 5. ç”Ÿæˆé¡¹ç›®è®¡åˆ’\n",
        "prompt = \"\"\"è¯·ä¸º\"å¼€å‘ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\"é¡¹ç›®åˆ¶å®šè®¡åˆ’ã€‚\n",
        "\n",
        "è¦æ±‚ï¼š\n",
        "1. åˆ—å‡º 4-6 ä¸ªä¸»è¦ä»»åŠ¡\n",
        "2. è®¾ç½®åˆç†çš„ä¼˜å…ˆçº§ï¼ˆ1-5ï¼‰\n",
        "3. ä¼°ç®—æˆªæ­¢æ—¥æœŸ\n",
        "4. æ·»åŠ ç›¸å…³æ ‡ç­¾\n",
        "\"\"\"\n",
        "\n",
        "result = structured_model.invoke(prompt)\n",
        "\n",
        "# 6. è¾“å‡ºç»“æœ\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"é¡¹ç›®ï¼š{result.project_name}\")\n",
        "print(f\"æè¿°ï¼š{result.description}\")\n",
        "print(f\"é¢„è®¡å·¥æœŸï¼š{result.total_days} å¤©\")\n",
        "print(f\"\\nä»»åŠ¡åˆ—è¡¨ï¼š\")\n",
        "\n",
        "for i, task in enumerate(result.tasks, 1):\n",
        "    print(f\"\\n{i}. {task.title}\")\n",
        "    print(f\"   ä¼˜å…ˆçº§ï¼š{'â­' * task.priority}\")\n",
        "    print(f\"   æˆªæ­¢æ—¥æœŸï¼š{task.deadline or 'æœªè®¾ç½®'}\")\n",
        "    print(f\"   æ ‡ç­¾ï¼š{', '.join(task.tags)}\")"
      ],
      "metadata": {
        "id": "dIiWtbVztmRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2ã€å¸¦é‡è¯•çš„ç»“æ„åŒ–è¾“å‡º\n",
        "\n",
        "å½“è¾“å‡ºéªŒè¯å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ã€‚"
      ],
      "metadata": {
        "id": "bMRkmTfquEXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import ValidationError\n",
        "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
        "from typing import Callable\n",
        "\n",
        "class StructuredOutputWithRetry:\n",
        "    \"\"\"å¸¦é‡è¯•çš„ç»“æ„åŒ–è¾“å‡º\"\"\"\n",
        "\n",
        "    def __init__(self, schema: type[BaseModel], max_retries: int = 3):\n",
        "        self.schema = schema\n",
        "        self.max_retries = max_retries\n",
        "\n",
        "    def create_hook(self):\n",
        "        @wrap_model_call\n",
        "        def retry_on_validation_error(\n",
        "            request: ModelRequest,\n",
        "            handler: Callable[[ModelRequest], ModelResponse]\n",
        "        ) -> ModelResponse:\n",
        "            \"\"\"éªŒè¯å¤±è´¥æ—¶é‡è¯•\"\"\"\n",
        "\n",
        "            for attempt in range(self.max_retries):\n",
        "                try:\n",
        "                    response = handler(request)\n",
        "\n",
        "                    # å°è¯•éªŒè¯\n",
        "                    if hasattr(response, 'content') and isinstance(response.content, dict):\n",
        "                        self.schema(**response.content)  # éªŒè¯\n",
        "                        print(f\"âœ… éªŒè¯é€šè¿‡ï¼ˆå°è¯• {attempt + 1}ï¼‰\")\n",
        "\n",
        "                    return response\n",
        "\n",
        "                except ValidationError as e:\n",
        "                    print(f\"âš ï¸  éªŒè¯å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{self.max_retries}ï¼‰\")\n",
        "                    print(f\"   é”™è¯¯ï¼š{e}\")\n",
        "\n",
        "                    if attempt < self.max_retries - 1:\n",
        "                        # åœ¨æç¤ºè¯ä¸­æ·»åŠ é”™è¯¯ä¿¡æ¯\n",
        "                        error_msg = f\"\\n\\næ³¨æ„ï¼šä¸Šæ¬¡è¾“å‡ºæ ¼å¼æœ‰è¯¯ï¼š{e}\\nè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºã€‚\"\n",
        "\n",
        "                        # ä¿®æ”¹ç³»ç»Ÿæ¶ˆæ¯\n",
        "                        new_system = request.system_message.content + error_msg if request.system_message else error_msg\n",
        "                        modified_request = request.override(\n",
        "                            system_message=SystemMessage(content=new_system)\n",
        "                        )\n",
        "                        request = modified_request\n",
        "                    else:\n",
        "                        raise\n",
        "\n",
        "            return response\n",
        "\n",
        "        return retry_on_validation_error\n",
        "\n",
        "# ä½¿ç”¨\n",
        "retry_handler = StructuredOutputWithRetry(schema=ProjectPlan, max_retries=3)\n",
        "\n",
        "agent = create_agent(\n",
        "    model=structured_model,\n",
        "    middleware=[retry_handler.create_hook()],\n",
        "    tools=[]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "59UXIRjnuHpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# å®æˆ˜é¡¹ç›® - ç”Ÿäº§çº§æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹\n",
        "\n",
        "###  é¡¹ç›®éœ€æ±‚\n",
        "\n",
        "æ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œå…·å¤‡ï¼š\n",
        "\n",
        "1. âœ… **å¤šæ­¥æ¨ç†** - åˆ†è§£å¤æ‚é—®é¢˜\n",
        "2. âœ… **ä¿¡æ¯æœç´¢** - æœç´¢ç›¸å…³èµ„æ–™\n",
        "3. âœ… **æ€§èƒ½ç›‘æ§** - è¿½è¸ªè€—æ—¶å’Œæˆæœ¬\n",
        "4. âœ… **æˆæœ¬ä¼˜åŒ–** - åŠ¨æ€æ¨¡å‹é€‰æ‹©\n",
        "5. âœ… **é”™è¯¯æ¢å¤** - è‡ªåŠ¨é‡è¯•å’Œé™çº§\n",
        "6. âœ… **ç»“æ„åŒ–è¾“å‡º** - ç”Ÿæˆæ ‡å‡†åŒ–æŠ¥å‘Š\n",
        "7. âœ… **æ•æ„Ÿä¿¡æ¯ä¿æŠ¤** - æ£€æµ‹å’Œå¤„ç† PII"
      ],
      "metadata": {
        "id": "qzs9eVJLvJVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "å®æˆ˜é¡¹ç›®ï¼šç”Ÿäº§çº§æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼ˆLangChain 1.0ï¼‰\n",
        "\n",
        "ä½¿ç”¨å®˜æ–¹ Middleware APIï¼ŒåŒ…å«ï¼š\n",
        "- æ€§èƒ½ç›‘æ§\n",
        "- æˆæœ¬è¿½è¸ª\n",
        "- é”™è¯¯é‡è¯•\n",
        "- ç»“æ„åŒ–è¾“å‡º\n",
        "- åŠ¨æ€æ¨¡å‹é€‰æ‹©\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import (\n",
        "    AgentMiddleware,\n",
        "    AgentState,\n",
        "    ModelRequest,\n",
        "    ModelResponse,\n",
        "    before_model,\n",
        "    after_model,\n",
        "    wrap_model_call,\n",
        "    wrap_tool_call,\n",
        "    SummarizationMiddleware,\n",
        "    ModelCallLimitMiddleware,\n",
        ")\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import AIMessage, SystemMessage\n",
        "from langchain.tools.tool_node import ToolCallRequest\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langgraph.types import Command\n",
        "from langgraph.runtime import Runtime\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Any, Callable\n",
        "from datetime import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "# ==================== æ•°æ®æ¨¡å‹ ====================\n",
        "\n",
        "class SubQuestion(BaseModel):\n",
        "    \"\"\"å­é—®é¢˜\"\"\"\n",
        "    question: str = Field(description=\"å­é—®é¢˜å†…å®¹\")\n",
        "    priority: int = Field(description=\"ä¼˜å…ˆçº§1-5\", ge=1, le=5)\n",
        "    difficulty: str = Field(description=\"éš¾åº¦ï¼šeasy/medium/hard\")\n",
        "\n",
        "class ResearchPlan(BaseModel):\n",
        "    \"\"\"ç ”ç©¶è®¡åˆ’\"\"\"\n",
        "    main_question: str = Field(description=\"ä¸»é—®é¢˜\")\n",
        "    sub_questions: List[SubQuestion] = Field(description=\"å­é—®é¢˜åˆ—è¡¨\")\n",
        "    approach: str = Field(description=\"ç ”ç©¶æ–¹æ³•\")\n",
        "    estimated_time: str = Field(description=\"é¢„è®¡è€—æ—¶\")\n",
        "\n",
        "class ResearchFinding(BaseModel):\n",
        "    \"\"\"ç ”ç©¶å‘ç°\"\"\"\n",
        "    topic: str = Field(description=\"ä¸»é¢˜\")\n",
        "    key_points: List[str] = Field(description=\"å…³é”®è¦ç‚¹\")\n",
        "    sources: List[str] = Field(description=\"ä¿¡æ¯æ¥æº\")\n",
        "    confidence: float = Field(description=\"ç½®ä¿¡åº¦0-1\", ge=0, le=1)\n",
        "\n",
        "class ResearchReport(BaseModel):\n",
        "    \"\"\"ç ”ç©¶æŠ¥å‘Š\"\"\"\n",
        "    title: str = Field(description=\"æŠ¥å‘Šæ ‡é¢˜\")\n",
        "    summary: str = Field(description=\"æ‰§è¡Œæ‘˜è¦\")\n",
        "    findings: List[ResearchFinding] = Field(description=\"ç ”ç©¶å‘ç°\")\n",
        "    conclusion: str = Field(description=\"ç»“è®º\")\n",
        "    recommendations: List[str] = Field(description=\"å»ºè®®\")\n",
        "    generated_at: str = Field(description=\"ç”Ÿæˆæ—¶é—´\")\n",
        "\n",
        "# ==================== å·¥å…·å®šä¹‰ ====================\n",
        "\n",
        "@tool\n",
        "def search_information(query: str) -> str:\n",
        "    \"\"\"æœç´¢ä¿¡æ¯ï¼ˆæ¨¡æ‹ŸçŸ¥è¯†åº“ï¼‰\n",
        "\n",
        "    Args:\n",
        "        query: æœç´¢å…³é”®è¯\n",
        "    \"\"\"\n",
        "    knowledge = {\n",
        "        \"LangChain\": \"LangChain 1.0 æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºã€Agent ç­‰ã€‚\",\n",
        "        \"Middleware\": \"Middleware æä¾›é’©å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬ before_modelã€after_modelã€wrap_model_call ç­‰ï¼Œç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹ã€‚\",\n",
        "        \"RAG\": \"RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆï¼Œé€šè¿‡å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œæé«˜å›ç­”å‡†ç¡®æ€§ã€‚\",\n",
        "        \"Agent\": \"Agent æ˜¯èƒ½å¤Ÿä½¿ç”¨å·¥å…·ã€è¿›è¡Œæ¨ç†çš„ AI ç³»ç»Ÿï¼ŒLangChain 1.0 æä¾›äº†å¼ºå¤§çš„ Agent æ¡†æ¶ã€‚\",\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for key, value in knowledge.items():\n",
        "        if query.lower() in key.lower() or key.lower() in query.lower():\n",
        "            results.append(f\"ã€{key}ã€‘{value}\")\n",
        "\n",
        "    return \"\\n\\n\".join(results) if results else f\"æœªæ‰¾åˆ°å…³äº'{query}'çš„ä¿¡æ¯\"\n",
        "\n",
        "@tool\n",
        "def analyze_data(data: str) -> str:\n",
        "    \"\"\"åˆ†ææ•°æ®\n",
        "\n",
        "    Args:\n",
        "        data: éœ€è¦åˆ†æçš„æ•°æ®\n",
        "    \"\"\"\n",
        "    word_count = len(data.split())\n",
        "    char_count = len(data)\n",
        "\n",
        "    return f\"\"\"æ•°æ®åˆ†æç»“æœï¼š\n",
        "- æ€»å­—æ•°ï¼š{word_count}\n",
        "- æ€»å­—ç¬¦æ•°ï¼š{char_count}\n",
        "- ä¿¡æ¯å®Œæ•´æ€§ï¼š{'é«˜' if word_count > 50 else 'ä¸­' if word_count > 20 else 'ä½'}\n",
        "- åŒ…å«å…³é”®æ¦‚å¿µæ•°ï¼š{len([k for k in [\"LangChain\", \"Middleware\", \"RAG\", \"Agent\"] if k in data])}\n",
        "\"\"\"\n",
        "\n",
        "# ==================== Middleware å®šä¹‰ ====================\n",
        "\n",
        "class PerformanceMonitorMiddleware(AgentMiddleware):\n",
        "    \"\"\"æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session_start = None\n",
        "        self.model_calls = []\n",
        "        self.tool_calls = []\n",
        "\n",
        "    def before_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        self.session_start = time.time()\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸš€ ä¼šè¯å¼€å§‹ï¼š{datetime.now().strftime('%H:%M:%S')}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        return None\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        start = time.time()\n",
        "        try:\n",
        "            response = handler(request)\n",
        "            elapsed = time.time() - start\n",
        "            self.model_calls.append(elapsed)\n",
        "            print(f\"â±ï¸  æ¨¡å‹è°ƒç”¨ #{len(self.model_calls)}ï¼š{elapsed:.2f}ç§’\")\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}\")\n",
        "            raise\n",
        "\n",
        "    def wrap_tool_call(\n",
        "        self,\n",
        "        request: ToolCallRequest,\n",
        "        handler: Callable[[ToolCallRequest], ToolMessage | Command]\n",
        "    ) -> ToolMessage | Command:\n",
        "        start = time.time()\n",
        "        tool_name = request.tool_call[\"name\"]\n",
        "        try:\n",
        "            result = handler(request)\n",
        "            elapsed = time.time() - start\n",
        "            self.tool_calls.append((tool_name, elapsed))\n",
        "            print(f\"ğŸ”§ å·¥å…·è°ƒç”¨ ({tool_name})ï¼š{elapsed:.2f}ç§’\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ å·¥å…·è°ƒç”¨å¤±è´¥ ({tool_name})ï¼š{e}\")\n",
        "            raise\n",
        "\n",
        "    def after_agent(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        total_time = time.time() - self.session_start\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ“Š æ€§èƒ½ç»Ÿè®¡\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’\")\n",
        "        print(f\"æ¨¡å‹è°ƒç”¨ï¼š{len(self.model_calls)} æ¬¡\")\n",
        "        if self.model_calls:\n",
        "            print(f\"  - å¹³å‡ï¼š{sum(self.model_calls)/len(self.model_calls):.2f}ç§’\")\n",
        "            print(f\"  - æ€»è®¡ï¼š{sum(self.model_calls):.2f}ç§’\")\n",
        "        print(f\"å·¥å…·è°ƒç”¨ï¼š{len(self.tool_calls)} æ¬¡\")\n",
        "        if self.tool_calls:\n",
        "            total_tool_time = sum(t[1] for t in self.tool_calls)\n",
        "            print(f\"  - æ€»è®¡ï¼š{total_tool_time:.2f}ç§’\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return None\n",
        "\n",
        "class CostTrackingMiddleware(AgentMiddleware):\n",
        "    \"\"\"æˆæœ¬è¿½è¸ªä¸­é—´ä»¶\"\"\"\n",
        "\n",
        "    def __init__(self, budget: float = 1.0):\n",
        "        self.budget = budget\n",
        "        self.total_cost = 0.0\n",
        "        self.model_prices = {\n",
        "            \"Qwen/Qwen3-8B\": 0.5,\n",
        "            \"Qwen/Qwen2.5-7B-Instruct\": 0.3,\n",
        "        }\n",
        "\n",
        "    def before_model(\n",
        "        self,\n",
        "        state: AgentState,\n",
        "        runtime: Runtime\n",
        "    ) -> dict[str, Any] | None:\n",
        "        if self.total_cost >= self.budget:\n",
        "            print(f\"âš ï¸  é¢„ç®—å·²ç”¨å®Œï¼ˆÂ¥{self.budget}ï¼‰\")\n",
        "            return {\n",
        "                \"jump_to\": \"end\",\n",
        "                \"messages\": [AIMessage(content=\"é¢„ç®—å·²ç”¨å®Œ\")]\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        response = handler(request)\n",
        "\n",
        "        # ä¼°ç®—æˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
        "        model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "        price = self.model_prices.get(model_name, 0.3)\n",
        "\n",
        "        # ä¼°ç®— tokensï¼ˆç®€åŒ–ï¼‰\n",
        "        input_tokens = sum(len(str(m.content)) for m in request.messages) * 1.5\n",
        "        print(response)\n",
        "        output_tokens = len(str(response.result[0].content)) * 1.5\n",
        "\n",
        "        cost = (input_tokens + output_tokens) / 1_000_000 * price\n",
        "        self.total_cost += cost\n",
        "\n",
        "        print(f\"ğŸ’° æˆæœ¬ï¼šÂ¥{cost:.4f}ï¼ˆç´¯è®¡ï¼šÂ¥{self.total_cost:.4f}/Â¥{self.budget}ï¼‰\")\n",
        "\n",
        "        return response\n",
        "\n",
        "class RetryMiddleware(AgentMiddleware):\n",
        "    \"\"\"é‡è¯•ä¸­é—´ä»¶\"\"\"\n",
        "\n",
        "    def __init__(self, max_retries: int = 2):\n",
        "        self.max_retries = max_retries\n",
        "\n",
        "    def wrap_model_call(\n",
        "        self,\n",
        "        request: ModelRequest,\n",
        "        handler: Callable[[ModelRequest], ModelResponse]\n",
        "    ) -> ModelResponse:\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            try:\n",
        "                return handler(request)\n",
        "            except Exception as e:\n",
        "                if attempt == self.max_retries:\n",
        "                    print(f\"âŒ é‡è¯•{self.max_retries}æ¬¡åä»å¤±è´¥ï¼š{e}\")\n",
        "                    raise\n",
        "\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"ğŸ”„ é‡è¯•ä¸­ï¼ˆ{attempt + 1}/{self.max_retries}ï¼‰ï¼Œ{wait_time}ç§’åé‡è¯•\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "# ==================== æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ ====================\n",
        "\n",
        "class ResearchAssistant:\n",
        "    \"\"\"æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # åˆ›å»ºæ¨¡å‹\n",
        "        self.model = init_chat_model(\n",
        "            \"Qwen/Qwen3-8B\",\n",
        "            model_provider=\"openai\",\n",
        "            base_url=\"https://api.siliconflow.cn/v1\",\n",
        "            api_key= API_KEY,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # å·¥å…·\n",
        "        self.tools = [search_information, analyze_data]\n",
        "\n",
        "        # Middleware\n",
        "        self.performance_monitor = PerformanceMonitorMiddleware()\n",
        "        self.cost_tracker = CostTrackingMiddleware(budget=0.5)\n",
        "        self.retry_handler = RetryMiddleware(max_retries=2)\n",
        "\n",
        "        # åˆ›å»º Agent\n",
        "        self.agent = create_agent(\n",
        "            model=self.model,\n",
        "            tools=self.tools,\n",
        "            middleware=[\n",
        "                self.cost_tracker,           # 1. æˆæœ¬æ§åˆ¶\n",
        "                ModelCallLimitMiddleware(run_limit=15),  # 2. è°ƒç”¨é™åˆ¶\n",
        "                self.retry_handler,          # 3. é‡è¯•\n",
        "                self.performance_monitor,    # 4. æ€§èƒ½ç›‘æ§\n",
        "            ],\n",
        "            system_prompt=\"\"\"ä½ æ˜¯ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚\n",
        "\n",
        "            èŒè´£ï¼š\n",
        "            1. åˆ†è§£å¤æ‚é—®é¢˜ä¸ºå­é—®é¢˜\n",
        "            2. ä½¿ç”¨å·¥å…·æœç´¢å’Œåˆ†æä¿¡æ¯\n",
        "            3. ç”Ÿæˆç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Š\n",
        "\n",
        "            å·¥ä½œæµç¨‹ï¼š\n",
        "            1. ç†è§£ç ”ç©¶ä¸»é¢˜\n",
        "            2. ä½¿ç”¨ search_information æœç´¢ç›¸å…³ä¿¡æ¯\n",
        "            3. ä½¿ç”¨ analyze_data åˆ†ææ•°æ®\n",
        "            4. ç»¼åˆä¿¡æ¯ç”Ÿæˆç»“è®º\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def create_plan(self, question: str) -> ResearchPlan:\n",
        "        \"\"\"åˆ¶å®šç ”ç©¶è®¡åˆ’\"\"\"\n",
        "        print(f\"ğŸ“‹ æ­£åœ¨åˆ¶å®šç ”ç©¶è®¡åˆ’...\\n\")\n",
        "\n",
        "        structured_model = self.model.with_structured_output(ResearchPlan)\n",
        "\n",
        "        prompt = f\"\"\"è¯·ä¸ºä»¥ä¸‹ç ”ç©¶é—®é¢˜åˆ¶å®šè¯¦ç»†è®¡åˆ’ï¼š\n",
        "\n",
        "        ç ”ç©¶é—®é¢˜ï¼š{question}\n",
        "\n",
        "        è¦æ±‚ï¼š\n",
        "        1. åˆ†è§£ä¸º 3-4 ä¸ªå­é—®é¢˜\n",
        "        2. è®¾ç½®ä¼˜å…ˆçº§ï¼ˆ1-5ï¼‰\n",
        "        3. è¯„ä¼°éš¾åº¦ï¼ˆeasy/medium/hardï¼‰\n",
        "        4. è¯´æ˜ç ”ç©¶æ–¹æ³•\n",
        "        5. ä¼°ç®—æ‰€éœ€æ—¶é—´\n",
        "        \"\"\"\n",
        "\n",
        "        plan = structured_model.invoke(prompt)\n",
        "        return plan\n",
        "\n",
        "    def research(self, sub_question: str) -> str:\n",
        "        \"\"\"ç ”ç©¶å•ä¸ªå­é—®é¢˜\"\"\"\n",
        "        print(f\"\\nğŸ” ç ”ç©¶å­é—®é¢˜ï¼š{sub_question}\\n\")\n",
        "\n",
        "        result = self.agent.invoke({\n",
        "            \"messages\": [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"è¯·æœç´¢å¹¶åˆ†æå…³äº'{sub_question}'çš„ä¿¡æ¯\"\n",
        "            }]\n",
        "        })\n",
        "\n",
        "        return result[\"messages\"][-1].content\n",
        "\n",
        "    def generate_report(\n",
        "        self,\n",
        "        question: str,\n",
        "        findings: List[str]\n",
        "    ) -> ResearchReport:\n",
        "        \"\"\"ç”Ÿæˆç ”ç©¶æŠ¥å‘Š\"\"\"\n",
        "        print(f\"\\nğŸ“ æ­£åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...\\n\")\n",
        "\n",
        "        structured_model = self.model.with_structured_output(ResearchReport)\n",
        "\n",
        "        findings_text = \"\\n\\n\".join([\n",
        "            f\"å‘ç° {i+1}ï¼š\\n{f}\"\n",
        "            for i, f in enumerate(findings)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"åŸºäºä»¥ä¸‹ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆå®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šï¼š\n",
        "\n",
        "        ç ”ç©¶é—®é¢˜ï¼š{question}\n",
        "\n",
        "        ç ”ç©¶å‘ç°ï¼š\n",
        "        {findings_text}\n",
        "\n",
        "        è¦æ±‚ï¼š\n",
        "        1. æ’°å†™æ‰§è¡Œæ‘˜è¦\n",
        "        2. æ•´ç†å…³é”®å‘ç°ï¼ˆæ¯ä¸ªå‘ç°åŒ…å«ä¸»é¢˜ã€è¦ç‚¹ã€æ¥æºã€ç½®ä¿¡åº¦ï¼‰\n",
        "        3. å¾—å‡ºç»“è®º\n",
        "        4. æä¾›å»ºè®®\n",
        "        \"\"\"\n",
        "\n",
        "        report = structured_model.invoke(prompt)\n",
        "        report.generated_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        return report\n",
        "\n",
        "    def conduct_research(self, question: str) -> ResearchReport:\n",
        "        \"\"\"æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ¯ å¼€å§‹ç ”ç©¶\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"\\nä¸»é¢˜ï¼š{question}\\n\")\n",
        "\n",
        "        # 1. åˆ¶å®šè®¡åˆ’\n",
        "        plan = self.create_plan(question)\n",
        "\n",
        "        print(f\"\\nâœ… è®¡åˆ’åˆ¶å®šå®Œæˆ\")\n",
        "        print(f\"ç ”ç©¶æ–¹æ³•ï¼š{plan.approach}\")\n",
        "        print(f\"é¢„è®¡è€—æ—¶ï¼š{plan.estimated_time}\")\n",
        "        print(f\"\\nå­é—®é¢˜ï¼š\")\n",
        "        for i, sq in enumerate(plan.sub_questions, 1):\n",
        "            print(f\"{i}. [{sq.difficulty.upper()}] {sq.question}\")\n",
        "\n",
        "        # 2. é€ä¸ªç ”ç©¶\n",
        "        findings = []\n",
        "        for i, sq in enumerate(plan.sub_questions, 1):\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"è¿›åº¦ï¼š{i}/{len(plan.sub_questions)}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            finding = self.research(sq.question)\n",
        "            findings.append(finding)\n",
        "\n",
        "        # 3. ç”ŸæˆæŠ¥å‘Š\n",
        "        report = self.generate_report(question, findings)\n",
        "\n",
        "        return report\n",
        "\n",
        "def main():\n",
        "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ”¬ æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼ˆLangChain 1.0 Middlewareï¼‰\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    assistant = ResearchAssistant()\n",
        "\n",
        "    # ç ”ç©¶ä¸»é¢˜\n",
        "    topic = \"LangChain 1.0 Middleware ç³»ç»Ÿçš„æ ¸å¿ƒç‰¹æ€§å’Œåº”ç”¨åœºæ™¯\"\n",
        "\n",
        "    # æ‰§è¡Œç ”ç©¶\n",
        "    report = assistant.conduct_research(topic)\n",
        "\n",
        "    # è¾“å‡ºæŠ¥å‘Š\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ“Š ç ”ç©¶æŠ¥å‘Š\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"æ ‡é¢˜ï¼š{report.title}\")\n",
        "    print(f\"\\næ‘˜è¦ï¼š\\n{report.summary}\")\n",
        "    print(f\"\\nç ”ç©¶å‘ç°ï¼š\")\n",
        "    for i, finding in enumerate(report.findings, 1):\n",
        "        print(f\"\\n{i}. {finding.topic}\")\n",
        "        print(f\"   ç½®ä¿¡åº¦ï¼š{finding.confidence:.0%}\")\n",
        "        for point in finding.key_points:\n",
        "            print(f\"   â€¢ {point}\")\n",
        "\n",
        "    print(f\"\\nç»“è®ºï¼š\\n{report.conclusion}\")\n",
        "\n",
        "    print(f\"\\nå»ºè®®ï¼š\")\n",
        "    for i, rec in enumerate(report.recommendations, 1):\n",
        "        print(f\"{i}. {rec}\")\n",
        "\n",
        "    print(f\"\\nç”Ÿæˆæ—¶é—´ï¼š{report.generated_at}\")\n",
        "\n",
        "    # ä¿å­˜æŠ¥å‘Š\n",
        "    filename = f\"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report.model_dump(), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ… æŠ¥å‘Šå·²ä¿å­˜ï¼š{filename}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0JAr429bvLEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa1baf95-dd02-4a77-b7e6-870f36581781"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ”¬ æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼ˆLangChain 1.0 Middlewareï¼‰\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ å¼€å§‹ç ”ç©¶\n",
            "======================================================================\n",
            "\n",
            "ä¸»é¢˜ï¼šLangChain 1.0 Middleware ç³»ç»Ÿçš„æ ¸å¿ƒç‰¹æ€§å’Œåº”ç”¨åœºæ™¯\n",
            "\n",
            "ğŸ“‹ æ­£åœ¨åˆ¶å®šç ”ç©¶è®¡åˆ’...\n",
            "\n",
            "\n",
            "âœ… è®¡åˆ’åˆ¶å®šå®Œæˆ\n",
            "ç ”ç©¶æ–¹æ³•ï¼šä¸ºç ”ç©¶é—®é¢˜ 'LangChain 1.0 Middleware ç³»ç»Ÿçš„æ ¸å¿ƒç‰¹æ€§å’Œåº”ç”¨åœºæ™¯' åˆ¶å®šè¯¦ç»†ç ”ç©¶è®¡åˆ’ï¼Œåˆ†è§£ä¸º 3-4 ä¸ªå­é—®é¢˜ï¼Œå¹¶è®¾ç½®ä¼˜å…ˆçº§ã€éš¾åº¦è¯„ä¼°ã€ç ”ç©¶æ–¹æ³•å’Œæ—¶é—´ä¼°ç®—ã€‚\n",
            "é¢„è®¡è€—æ—¶ï¼šçº¦ 10-15 å¤©\n",
            "\n",
            "å­é—®é¢˜ï¼š\n",
            "1. [MEDIUM] LangChain 1.0 Middleware çš„æ ¸å¿ƒæ¶æ„å’ŒåŠŸèƒ½æ¨¡å—æ˜¯ä»€ä¹ˆï¼Ÿ\n",
            "2. [MEDIUM] LangChain 1.0 Middleware æ”¯æŒå“ªäº›ç±»å‹çš„é›†æˆå’Œæ‰©å±•ï¼Ÿ\n",
            "3. [HARD] LangChain 1.0 Middleware åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­æœ‰å“ªäº›å…¸å‹ç”¨ä¾‹ï¼Ÿ\n",
            "4. [MEDIUM] LangChain 1.0 Middleware çš„æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯æ‰©å±•æ€§å¦‚ä½•ï¼Ÿ\n",
            "\n",
            "======================================================================\n",
            "è¿›åº¦ï¼š1/4\n",
            "======================================================================\n",
            "\n",
            "ğŸ” ç ”ç©¶å­é—®é¢˜ï¼šLangChain 1.0 Middleware çš„æ ¸å¿ƒæ¶æ„å’ŒåŠŸèƒ½æ¨¡å—æ˜¯ä»€ä¹ˆï¼Ÿ\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ ä¼šè¯å¼€å§‹ï¼š13:29:36\n",
            "======================================================================\n",
            "\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #1ï¼š1.56ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 357, 'total_tokens': 390, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0873f8c0fd8608b9d034ed9ed191', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0873-f6f4-7200-8a8b-f066fbb427ed-0', tool_calls=[{'name': 'search_information', 'args': {'query': 'LangChain 1.0 Middleware çš„æ ¸å¿ƒæ¶æ„å’ŒåŠŸèƒ½æ¨¡å—æ˜¯ä»€ä¹ˆï¼Ÿ'}, 'id': '019b0873fc7915fa7da4e8a1dbbc6842', 'type': 'tool_call'}], usage_metadata={'input_tokens': 357, 'output_tokens': 33, 'total_tokens': 390, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0000ï¼ˆç´¯è®¡ï¼šÂ¥0.0000/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (search_information)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #2ï¼š4.88ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 469, 'total_tokens': 555, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0873febebfe1d5f0654b902060dd', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0873-fd13-7b41-b79e-6485a23b5801-0', tool_calls=[{'name': 'analyze_data', 'args': {'data': 'ã€LangChainã€‘LangChain 1.0 æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºã€Agent ç­‰ã€‚\\n\\nã€Middlewareã€‘Middleware æä¾›é’©å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬ before_modelã€after_modelã€wrap_model_call ç­‰ï¼Œç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹ã€‚'}, 'id': '019b08740fb407b2b7ba2e7f87ab7287', 'type': 'tool_call'}], usage_metadata={'input_tokens': 469, 'output_tokens': 86, 'total_tokens': 555, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0001ï¼ˆç´¯è®¡ï¼šÂ¥0.0001/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (analyze_data)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #3ï¼š7.51ç§’\n",
            "ModelResponse(result=[AIMessage(content='LangChain 1.0 çš„ Middleware æ˜¯ä¸€ä¸ªç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹çš„é’©å­ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½æ¨¡å—ï¼š\\n\\n1. **before_model**ï¼šåœ¨æ¨¡å‹è°ƒç”¨å‰æ‰§è¡Œï¼Œå¯ç”¨äºé¢„å¤„ç†æˆ–æ—¥å¿—è®°å½•ã€‚\\n2. **after_model**ï¼šåœ¨æ¨¡å‹è°ƒç”¨åæ‰§è¡Œï¼Œå¯ç”¨äºåå¤„ç†æˆ–ç»“æœåˆ†æã€‚\\n3. **wrap_model_call**ï¼šåŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œå…è®¸å¯¹è°ƒç”¨è¿‡ç¨‹è¿›è¡Œæ›´ç»†ç²’åº¦çš„æ§åˆ¶ã€‚\\n\\nè¿™äº›æ¨¡å—ä¸ºå¼€å‘è€…æä¾›äº†çµæ´»çš„æ‰©å±•ç‚¹ï¼Œä»¥å¢å¼ºæˆ–ä¿®æ”¹ Agent çš„è¡Œä¸ºã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 607, 'total_tokens': 720, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b08741167a5982341a8f0beda832f', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0874-102c-7190-a41a-dacb0758aad7-0', usage_metadata={'input_tokens': 607, 'output_tokens': 113, 'total_tokens': 720, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0002ï¼ˆç´¯è®¡ï¼šÂ¥0.0003/Â¥0.5ï¼‰\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š æ€§èƒ½ç»Ÿè®¡\n",
            "======================================================================\n",
            "æ€»è€—æ—¶ï¼š13.97ç§’\n",
            "æ¨¡å‹è°ƒç”¨ï¼š3 æ¬¡\n",
            "  - å¹³å‡ï¼š4.65ç§’\n",
            "  - æ€»è®¡ï¼š13.96ç§’\n",
            "å·¥å…·è°ƒç”¨ï¼š2 æ¬¡\n",
            "  - æ€»è®¡ï¼š0.00ç§’\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "è¿›åº¦ï¼š2/4\n",
            "======================================================================\n",
            "\n",
            "ğŸ” ç ”ç©¶å­é—®é¢˜ï¼šLangChain 1.0 Middleware æ”¯æŒå“ªäº›ç±»å‹çš„é›†æˆå’Œæ‰©å±•ï¼Ÿ\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ ä¼šè¯å¼€å§‹ï¼š13:29:50\n",
            "======================================================================\n",
            "\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #4ï¼š3.09ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 358, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b08742f4cd240e371171fd6efff04', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-2d90-70a2-95ae-40c66373662e-0', tool_calls=[{'name': 'search_information', 'args': {'query': 'LangChain 1.0 Middleware æ”¯æŒå“ªäº›ç±»å‹çš„é›†æˆå’Œæ‰©å±•ï¼Ÿ'}, 'id': '019b087438569863f4b68be935922d6c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 358, 'output_tokens': 34, 'total_tokens': 392, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0000ï¼ˆç´¯è®¡ï¼šÂ¥0.0004/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (search_information)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #5ï¼š4.70ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 471, 'total_tokens': 557, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b08743af24e74f91e5f19eba079ac', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-39a1-7232-8667-1c7fe2bfbc20-0', tool_calls=[{'name': 'analyze_data', 'args': {'data': 'ã€LangChainã€‘LangChain 1.0 æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºã€Agent ç­‰ã€‚\\n\\nã€Middlewareã€‘Middleware æä¾›é’©å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬ before_modelã€after_modelã€wrap_model_call ç­‰ï¼Œç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹ã€‚'}, 'id': '019b08744b771c5a8b1ea5e108b260f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 471, 'output_tokens': 86, 'total_tokens': 557, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0001ï¼ˆç´¯è®¡ï¼šÂ¥0.0005/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (analyze_data)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #6ï¼š7.10ç§’\n",
            "ModelResponse(result=[AIMessage(content='LangChain 1.0 çš„ Middleware æ”¯æŒä»¥ä¸‹ç±»å‹çš„é›†æˆå’Œæ‰©å±•ï¼š\\n\\n1. **é’©å­ç³»ç»Ÿ**ï¼šåŒ…æ‹¬ `before_model`ã€`after_model` å’Œ `wrap_model_call` ç­‰é’©å­ï¼Œç”¨äºåœ¨æ¨¡å‹è°ƒç”¨å‰åæ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼Œä»è€Œæ§åˆ¶ Agent çš„æ‰§è¡Œæµç¨‹ã€‚\\n2. **ç»“æ„åŒ–è¾“å‡º**ï¼šå…è®¸å¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œä¾¿äºåç»­çš„ä½¿ç”¨å’Œè§£æã€‚\\n3. **Agent æ‰©å±•**ï¼šé€šè¿‡ Middleware å¯ä»¥æ‰©å±• Agent çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ·»åŠ æ—¥å¿—è®°å½•ã€ç›‘æ§ã€ç¼“å­˜ç­‰ã€‚\\n\\nè¿™äº›ç‰¹æ€§ä½¿å¾— LangChain 1.0 çš„ Middleware åœ¨æ„å»ºå’Œç®¡ç† LLM åº”ç”¨æ—¶éå¸¸çµæ´»å’Œå¼ºå¤§ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 609, 'total_tokens': 756, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b087450093528f6ee8320288fd497', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0874-4c02-7cc3-8c72-c7a8e1a55f55-0', usage_metadata={'input_tokens': 609, 'output_tokens': 147, 'total_tokens': 756, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0003ï¼ˆç´¯è®¡ï¼šÂ¥0.0007/Â¥0.5ï¼‰\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š æ€§èƒ½ç»Ÿè®¡\n",
            "======================================================================\n",
            "æ€»è€—æ—¶ï¼š14.90ç§’\n",
            "æ¨¡å‹è°ƒç”¨ï¼š6 æ¬¡\n",
            "  - å¹³å‡ï¼š4.81ç§’\n",
            "  - æ€»è®¡ï¼š28.85ç§’\n",
            "å·¥å…·è°ƒç”¨ï¼š4 æ¬¡\n",
            "  - æ€»è®¡ï¼š0.00ç§’\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "è¿›åº¦ï¼š3/4\n",
            "======================================================================\n",
            "\n",
            "ğŸ” ç ”ç©¶å­é—®é¢˜ï¼šLangChain 1.0 Middleware åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­æœ‰å“ªäº›å…¸å‹ç”¨ä¾‹ï¼Ÿ\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ ä¼šè¯å¼€å§‹ï¼š13:30:05\n",
            "======================================================================\n",
            "\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #7ï¼š1.70ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 358, 'total_tokens': 388, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b08746915b839bfaaa8bb39951338', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-67c5-7ff3-82ae-0ea5ade523ed-0', tool_calls=[{'name': 'search_information', 'args': {'query': 'LangChain 1.0 Middleware å…¸å‹ç”¨ä¾‹'}, 'id': '019b08746de6bd1aad5a7fb8651b2a6c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 358, 'output_tokens': 30, 'total_tokens': 388, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0000ï¼ˆç´¯è®¡ï¼šÂ¥0.0007/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (search_information)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #8ï¼š5.58ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 467, 'total_tokens': 553, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0874702b7c9e1a7f28e314a5952c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-6e6e-7a02-bed3-220a3eaf47aa-0', tool_calls=[{'name': 'analyze_data', 'args': {'data': 'ã€LangChainã€‘LangChain 1.0 æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºã€Agent ç­‰ã€‚\\n\\nã€Middlewareã€‘Middleware æä¾›é’©å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬ before_modelã€after_modelã€wrap_model_call ç­‰ï¼Œç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹ã€‚'}, 'id': '019b087481390c33571e14f83958e91e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 467, 'output_tokens': 86, 'total_tokens': 553, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0001ï¼ˆç´¯è®¡ï¼šÂ¥0.0008/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (analyze_data)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #9ï¼š11.58ç§’\n",
            "ModelResponse(result=[AIMessage(content='LangChain 1.0 çš„ Middleware æ˜¯ä¸€ä¸ªç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹çš„é’©å­ç³»ç»Ÿï¼Œå®ƒæä¾›äº†å‡ ä¸ªå…³é”®çš„é’©å­ï¼ŒåŒ…æ‹¬ `before_model`ã€`after_model` å’Œ `wrap_model_call`ã€‚è¿™äº›é’©å­å¯ä»¥ç”¨äºä»¥ä¸‹å…¸å‹ç”¨ä¾‹ï¼š\\n\\n1. **æ—¥å¿—è®°å½•**ï¼šåœ¨æ¨¡å‹è°ƒç”¨å‰åè®°å½•æ—¥å¿—ï¼Œä»¥ä¾¿è·Ÿè¸ªå’Œè°ƒè¯• Agent çš„æ‰§è¡Œè¿‡ç¨‹ã€‚\\n2. **æ€§èƒ½ç›‘æ§**ï¼šåœ¨æ¨¡å‹è°ƒç”¨å‰åè®°å½•æ—¶é—´æˆ³ï¼Œç”¨äºåˆ†ææ¨¡å‹æ‰§è¡Œçš„æ€§èƒ½è¡¨ç°ã€‚\\n3. **æ¨¡å‹è°ƒç”¨åŒ…è£…**ï¼šé€šè¿‡ `wrap_model_call` é’©å­ï¼Œå¯ä»¥å¯¹æ¨¡å‹è°ƒç”¨è¿›è¡ŒåŒ…è£…ï¼Œä¾‹å¦‚æ·»åŠ ç¼“å­˜ã€é‡è¯•é€»è¾‘æˆ–èº«ä»½éªŒè¯ã€‚\\n\\nè¿™äº›ç”¨ä¾‹å±•ç¤ºäº† Middleware åœ¨å®é™…åº”ç”¨ä¸­å¦‚ä½•å¢å¼º Agent çš„åŠŸèƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 165, 'prompt_tokens': 605, 'total_tokens': 770, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b08748b9aeb4bedae7174051502bc', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0874-843b-7413-ae2c-2da0f0402c8a-0', usage_metadata={'input_tokens': 605, 'output_tokens': 165, 'total_tokens': 770, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0003ï¼ˆç´¯è®¡ï¼šÂ¥0.0011/Â¥0.5ï¼‰\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š æ€§èƒ½ç»Ÿè®¡\n",
            "======================================================================\n",
            "æ€»è€—æ—¶ï¼š18.87ç§’\n",
            "æ¨¡å‹è°ƒç”¨ï¼š9 æ¬¡\n",
            "  - å¹³å‡ï¼š5.30ç§’\n",
            "  - æ€»è®¡ï¼š47.71ç§’\n",
            "å·¥å…·è°ƒç”¨ï¼š6 æ¬¡\n",
            "  - æ€»è®¡ï¼š0.00ç§’\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "è¿›åº¦ï¼š4/4\n",
            "======================================================================\n",
            "\n",
            "ğŸ” ç ”ç©¶å­é—®é¢˜ï¼šLangChain 1.0 Middleware çš„æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯æ‰©å±•æ€§å¦‚ä½•ï¼Ÿ\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ ä¼šè¯å¼€å§‹ï¼š13:30:23\n",
            "======================================================================\n",
            "\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #10ï¼š3.11ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 359, 'total_tokens': 394, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0874b5b7d3073b225aa6e85491a0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-b17b-73a0-8410-9e654e476f59-0', tool_calls=[{'name': 'search_information', 'args': {'query': 'LangChain 1.0 Middleware çš„æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯æ‰©å±•æ€§å¦‚ä½•ï¼Ÿ'}, 'id': '019b0874bd1df41ab0cf33d95c1ee96d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 359, 'output_tokens': 35, 'total_tokens': 394, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0000ï¼ˆç´¯è®¡ï¼šÂ¥0.0011/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (search_information)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #11ï¼š3.59ç§’\n",
            "ModelResponse(result=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 473, 'total_tokens': 559, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0874c00b005e20a6fc88ed021076', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b0874-bda7-7901-9154-3b38a296bc45-0', tool_calls=[{'name': 'analyze_data', 'args': {'data': 'ã€LangChainã€‘LangChain 1.0 æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºã€Agent ç­‰ã€‚\\n\\nã€Middlewareã€‘Middleware æä¾›é’©å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬ before_modelã€after_modelã€wrap_model_call ç­‰ï¼Œç”¨äºæ§åˆ¶ Agent æ‰§è¡Œæµç¨‹ã€‚'}, 'id': '019b0874cb1626d221e6fd32dae11dc8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 473, 'output_tokens': 86, 'total_tokens': 559, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0001ï¼ˆç´¯è®¡ï¼šÂ¥0.0012/Â¥0.5ï¼‰\n",
            "ğŸ”§ å·¥å…·è°ƒç”¨ (analyze_data)ï¼š0.00ç§’\n",
            "â±ï¸  æ¨¡å‹è°ƒç”¨ #12ï¼š5.46ç§’\n",
            "ModelResponse(result=[AIMessage(content='å…³äº LangChain 1.0 çš„ Middlewareï¼Œç›®å‰çš„ä¿¡æ¯è¾ƒä¸ºæœ‰é™ï¼Œä»…æåŠäº†å…¶ä½œä¸ºæ„å»º LLM åº”ç”¨æ¡†æ¶çš„ä¸€éƒ¨åˆ†ï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ Middlewareã€ç»“æ„åŒ–è¾“å‡ºå’Œ Agentã€‚Middleware æä¾›äº†é’©å­ç³»ç»Ÿï¼Œå¦‚ `before_model`ã€`after_model` å’Œ `wrap_model_call`ï¼Œç”¨äºæ§åˆ¶ Agent çš„æ‰§è¡Œæµç¨‹ã€‚\\n\\nç”±äºä¿¡æ¯å®Œæ•´æ€§è¾ƒä½ï¼Œå»ºè®®è¿›ä¸€æ­¥æŸ¥æ‰¾è¯¦ç»†èµ„æ–™æˆ–å®˜æ–¹æ–‡æ¡£ï¼Œä»¥è·å–å…³äº LangChain 1.0 Middleware åœ¨æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å…·ä½“åˆ†æå’Œè¯„ä»·ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 611, 'total_tokens': 721, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '019b0874cdb5b5cdb812b74d3a249464', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0874-cbb2-7f21-b5be-2b3b0bbcc1e1-0', usage_metadata={'input_tokens': 611, 'output_tokens': 110, 'total_tokens': 721, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})], structured_response=None)\n",
            "ğŸ’° æˆæœ¬ï¼šÂ¥0.0002ï¼ˆç´¯è®¡ï¼šÂ¥0.0015/Â¥0.5ï¼‰\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š æ€§èƒ½ç»Ÿè®¡\n",
            "======================================================================\n",
            "æ€»è€—æ—¶ï¼š12.18ç§’\n",
            "æ¨¡å‹è°ƒç”¨ï¼š12 æ¬¡\n",
            "  - å¹³å‡ï¼š4.99ç§’\n",
            "  - æ€»è®¡ï¼š59.88ç§’\n",
            "å·¥å…·è°ƒç”¨ï¼š8 æ¬¡\n",
            "  - æ€»è®¡ï¼š0.01ç§’\n",
            "======================================================================\n",
            "\n",
            "\n",
            "ğŸ“ æ­£åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...\n",
            "\n"
          ]
        },
        {
          "ename": "LengthFinishReasonError",
          "evalue": "Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=4096, prompt_tokens=663, total_tokens=4759, completion_tokens_details=None, prompt_tokens_details=None)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLengthFinishReasonError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-85540638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-85540638.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# æ‰§è¡Œç ”ç©¶\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massistant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconduct_research\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# è¾“å‡ºæŠ¥å‘Š\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-85540638.py\u001b[0m in \u001b[0;36mconduct_research\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# 3. ç”ŸæˆæŠ¥å‘Š\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfindings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-85540638.py\u001b[0m in \u001b[0;36mgenerate_report\u001b[0;34m(self, question, findings)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3141\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5546\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5547\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5548\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5549\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m         if (\n\u001b[1;32m   1372\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m                         )\n\u001b[1;32m   1341\u001b[0m                     )\n\u001b[0;32m-> 1342\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                     \u001b[0m_handle_openai_bad_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mparser\u001b[0;34m(raw_completion)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_completion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatCompletion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParsedChatCompletion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResponseFormatT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             return _parse_chat_completion(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mchat_completion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_completion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/lib/_parsing/_completions.py\u001b[0m in \u001b[0;36mparse_chat_completion\u001b[0;34m(response_format, input_tools, chat_completion)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchat_completion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_reason\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLengthFinishReasonError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_reason\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"content_filter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLengthFinishReasonError\u001b[0m: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=4096, prompt_tokens=663, total_tokens=4759, completion_tokens_details=None, prompt_tokens_details=None)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFNIFNz5c8RF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}