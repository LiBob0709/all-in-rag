{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiBob0709/all-in-rag/blob/main/05LangChain%201.0-RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHGJPSFhwpfG",
        "outputId": "395a34a2-a334-4ae0-852f-ff3b11a0e61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting chroma\n",
            "  Downloading Chroma-0.2.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting langchain-core<2.0.0,>=1.1.2 (from langchain)\n",
            "  Downloading langchain_core-1.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.9.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.55)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.12)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading langchain-1.1.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.4.1-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.3-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: chroma\n",
            "  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma: filename=Chroma-0.2.0-py3-none-any.whl size=7095 sha256=3e0820250eca9523acad7b47c51e050b0cf5e4f8520902711f50082d1a53b644\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/e1/e1/734dc19f0803917975827d330776fe4a9bedc1c7f324a1d70c\n",
            "Successfully built chroma\n",
            "Installing collected packages: chroma, requests, python-docx, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.1\n",
            "    Uninstalling langchain-core-1.1.1:\n",
            "      Successfully uninstalled langchain-core-1.1.1\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.1.2\n",
            "    Uninstalling langchain-1.1.2:\n",
            "      Successfully uninstalled langchain-1.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chroma-0.2.0 dataclasses-json-0.6.7 faiss-cpu-1.13.1 langchain-1.1.3 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.3 langchain-openai-1.1.1 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-6.4.1 python-docx-1.2.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain langchain-openai langchain-community pypdf python-docx faiss-cpu chroma sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2sZ4uMYKxw5L"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('DEEPSEEK_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Kd-xTrx-qX"
      },
      "source": [
        "# åŠ è½½ä¸åŒæ ¼å¼æ–‡æ¡£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "02Azkt8-x2LL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "fa1effc2-345b-4c28-88a4-4eb043565887"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File path company_report.pdf is not a valid file or url",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-307652096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# åŠ è½½ PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpdf_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"company_report.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpdf_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0maload\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0mto\u001b[0m \u001b[0mretrieve\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         self.parser = PyPDFParser(\n\u001b[1;32m    283\u001b[0m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path company_report.pdf is not a valid file or url"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "æ–‡æ¡£åŠ è½½ç¤ºä¾‹\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,        # PDF\n",
        "    Docx2txtLoader,     # Word\n",
        "    TextLoader,         # çº¯æ–‡æœ¬\n",
        "    DirectoryLoader     # æ‰¹é‡åŠ è½½\n",
        ")\n",
        "\n",
        "# åŠ è½½ PDF\n",
        "pdf_loader = PyPDFLoader(\"company_report.pdf\")\n",
        "pdf_docs = pdf_loader.load()\n",
        "\n",
        "print(f\"åŠ è½½äº† {len(pdf_docs)} é¡µ PDF\")\n",
        "print(f\"ç¬¬ä¸€é¡µå†…å®¹é¢„è§ˆ: {pdf_docs[0].page_content[:100]}...\")\n",
        "\n",
        "# åŠ è½½ Word\n",
        "docx_loader = Docx2txtLoader(\"policy.docx\")\n",
        "docx_docs = docx_loader.load()\n",
        "\n",
        "# åŠ è½½æ–‡æœ¬æ–‡ä»¶\n",
        "txt_loader = TextLoader(\"readme.txt\", encoding=\"utf-8\")\n",
        "txt_docs = txt_loader.load()\n",
        "\n",
        "# æ‰¹é‡åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£\n",
        "dir_loader = DirectoryLoader(\n",
        "    \"docs/\",\n",
        "    glob=\"**/*.txt\",  # åŒ¹é…æ‰€æœ‰ .txt æ–‡ä»¶\n",
        "    loader_cls=TextLoader,\n",
        "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
        ")\n",
        "all_docs = dir_loader.load()\n",
        "\n",
        "print(f\"ç›®å½•ä¸‹å…±åŠ è½½ {len(all_docs)} ä¸ªæ–‡æ¡£\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMwJuzyWyD_2"
      },
      "source": [
        "# Document å¯¹è±¡ç»“æ„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzvw7e8OyFTb"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "doc = Document(\n",
        "    page_content=\"è¿™æ˜¯æ–‡æ¡£å†…å®¹...\",\n",
        "    metadata={\n",
        "        \"source\": \"report.pdf\",\n",
        "        \"page\": 1,\n",
        "        \"author\": \"å¼ ä¸‰\",\n",
        "        \"date\": \"2024-01-15\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(doc.page_content)  # å†…å®¹\n",
        "print(doc.metadata)      # å…ƒæ•°æ®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaleL7myg6Z"
      },
      "source": [
        "## æ–‡æœ¬åˆ†å—ï¼ˆChunkingï¼‰\n",
        "\n",
        "###  ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ\n",
        "\n",
        "åŸå› ï¼š\n",
        "\n",
        "1. **é•¿æ–‡æ¡£é—®é¢˜**ï¼šæ¨¡å‹æœ‰ token é™åˆ¶\n",
        "2. **æ£€ç´¢ç²¾åº¦**ï¼šå°å—æ–‡æœ¬æ›´å®¹æ˜“åŒ¹é…\n",
        "3. **æˆæœ¬æ§åˆ¶**ï¼šå‡å°‘ä¸å¿…è¦çš„ token æ¶ˆè€—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpzRWBoryiqb"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters  import (\n",
        "    RecursiveCharacterTextSplitter,  # é€’å½’åˆ†å—ï¼ˆæ¨èï¼‰\n",
        "    CharacterTextSplitter,           # å­—ç¬¦åˆ†å—\n",
        "    TokenTextSplitter               # Token åˆ†å—\n",
        ")\n",
        "\n",
        "# ç­–ç•¥1ï¼šé€’å½’å­—ç¬¦åˆ†å—ï¼ˆæ¨èï¼‰\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,        # æ¯å—æœ€å¤§å­—ç¬¦æ•°\n",
        "    chunk_overlap=50,      # å—ä¹‹é—´é‡å å­—ç¬¦æ•°\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \" \", \"\"]  # åˆ†éš”ç¬¦ä¼˜å…ˆçº§\n",
        ")\n",
        "\n",
        "text = \"\"\"\n",
        "ç¬¬ä¸€æ®µå†…å®¹...\n",
        "\n",
        "ç¬¬äºŒæ®µå†…å®¹...\n",
        "\n",
        "ç¬¬ä¸‰æ®µå†…å®¹...\n",
        "\"\"\"\n",
        "\n",
        "chunks = splitter.split_text(text)\n",
        "print(f\"åˆ†æˆ {len(chunks)} å—\")\n",
        "\n",
        "# ç­–ç•¥2ï¼šåˆ†å‰²æ–‡æ¡£å¯¹è±¡\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"é•¿æ–‡æ¡£å†…å®¹...\", metadata={\"source\": \"doc1.txt\"}),\n",
        "    Document(page_content=\"å¦ä¸€ä¸ªæ–‡æ¡£...\", metadata={\"source\": \"doc2.txt\"})\n",
        "]\n",
        "\n",
        "split_docs = splitter.split_documents(docs)\n",
        "print(f\"åˆ†æˆ {len(split_docs)} ä¸ªæ–‡æ¡£å—\")\n",
        "\n",
        "# æ¯ä¸ªå—ä¿ç•™åŸå§‹å…ƒæ•°æ®\n",
        "for doc in split_docs[:3]:\n",
        "    print(f\"æ¥æº: {doc.metadata['source']}\")\n",
        "    print(f\"å†…å®¹: {doc.page_content[:50]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9aiKnzlzUIO"
      },
      "source": [
        "# ä¸­æ–‡åˆ†å—çš„æŠ€å·§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8v427bKzXuG"
      },
      "outputs": [],
      "source": [
        "# ä¸­æ–‡ä¸“ç”¨åˆ†å—å™¨\n",
        "chinese_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\n",
        "        \"\\n\\n\",      # æ®µè½\n",
        "        \"\\n\",        # æ¢è¡Œ\n",
        "        \"ã€‚\",        # å¥å·\n",
        "        \"ï¼\",        # æ„Ÿå¹å·\n",
        "        \"ï¼Ÿ\",        # é—®å·\n",
        "        \"ï¼›\",        # åˆ†å·\n",
        "        \"ï¼Œ\",        # é€—å·\n",
        "        \" \",         # ç©ºæ ¼\n",
        "        \"\"          # å­—ç¬¦\n",
        "    ],\n",
        "    length_function=len  # ä½¿ç”¨å­—ç¬¦æ•°\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDs8hOhc5X4C"
      },
      "source": [
        "# åˆ†å—å‚æ•°é€‰æ‹©\n",
        "| å‚æ•°              | æ¨èå€¼                | è¯´æ˜               |\n",
        "| ----------------- | --------------------- | ------------------ |\n",
        "| **chunk_size**    | 300-1000 å­—ç¬¦         | ä¸­æ–‡çº¦ 150-500 å­—  |\n",
        "| **chunk_overlap** | 10%-20% çš„ chunk_size | é¿å…è¯­ä¹‰æ–­è£‚       |\n",
        "| **separators**    | æŒ‰è¯­ä¹‰å±‚æ¬¡            | ä¼˜å…ˆæ®µè½ï¼Œå…¶æ¬¡å¥å­ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuuDSRDa5bz8"
      },
      "outputs": [],
      "source": [
        "# çŸ­æ–‡æœ¬ï¼ˆFAQã€é—®ç­”ï¼‰\n",
        "faq_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "# é•¿æ–‡æ¡£ï¼ˆè®ºæ–‡ã€æŠ¥å‘Šï¼‰\n",
        "report_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "# ä»£ç æ–‡æ¡£\n",
        "code_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\nclass \", \"\\n\\ndef \", \"\\n\\n\", \"\\n\", \" \"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwWtwerXbGw"
      },
      "source": [
        "Embedding å°†æ–‡æœ¬è½¬æ¢ä¸º**é«˜ç»´å‘é‡**ï¼ˆæ•°å­—æ•°ç»„ï¼‰ï¼š\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "æ–‡æœ¬: \"äººå·¥æ™ºèƒ½å‘å±•è¿…é€Ÿ\"\n",
        "      â†“\n",
        "å‘é‡: [0.23, -0.45, 0.67, ..., 0.12]  # é€šå¸¸ 768 æˆ– 1536 ç»´\n",
        "```\n",
        "**ç›¸ä¼¼æ–‡æœ¬çš„å‘é‡åœ¨ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ï¼š**\n",
        "\n",
        "\n",
        "```\n",
        "\"äººå·¥æ™ºèƒ½\" â†’ [0.2, 0.8, ...]\n",
        "\"AIæŠ€æœ¯\"   â†’ [0.3, 0.7, ...]  â† ç›¸ä¼¼ï¼Œè·ç¦»è¿‘\n",
        "\n",
        "\"ä»Šå¤©å¤©æ°”\" â†’ [0.9, 0.1, ...]  â† ä¸ç›¸ä¼¼ï¼Œè·ç¦»è¿œ\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfEHuQbJXbgd",
        "outputId": "34e6ae6c-73b2-4d94-8e8a-e11306be346c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å‘é‡ç»´åº¦: 1024\n",
            "å‘é‡å‰5ä¸ªå€¼: [-0.017258409410715103, 0.020045343786478043, -0.027274254709482193, -0.006901141721755266, -0.043919552117586136]\n",
            "æ‰¹é‡å‘é‡åŒ– 3 ä¸ªæ–‡æœ¬\n",
            "\n",
            "ç›¸ä¼¼åº¦:\n",
            "'äººå·¥æ™ºèƒ½' vs 'AIæŠ€æœ¯': 0.8056\n",
            "'äººå·¥æ™ºèƒ½' vs 'ä»Šå¤©å¤©æ°”': 0.5274\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Embedding ç¤ºä¾‹\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# å‘é‡åŒ–å•ä¸ªæ–‡æœ¬\n",
        "text = \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ\"\n",
        "vector = embeddings.embed_query(text)\n",
        "\n",
        "print(f\"å‘é‡ç»´åº¦: {len(vector)}\")\n",
        "print(f\"å‘é‡å‰5ä¸ªå€¼: {vector[:5]}\")\n",
        "\n",
        "# æ‰¹é‡å‘é‡åŒ–\n",
        "texts = [\n",
        "    \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯\",\n",
        "    \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ\",\n",
        "    \"ä»Šå¤©å¤©æ°”ä¸é”™\"\n",
        "]\n",
        "\n",
        "vectors = embeddings.embed_documents(texts)\n",
        "print(f\"æ‰¹é‡å‘é‡åŒ– {len(vectors)} ä¸ªæ–‡æœ¬\")\n",
        "# è®¡ç®—ç›¸ä¼¼åº¦\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "vec1 = embeddings.embed_query(\"äººå·¥æ™ºèƒ½\")\n",
        "vec2 = embeddings.embed_query(\"AIæŠ€æœ¯\")\n",
        "vec3 = embeddings.embed_query(\"ä»Šå¤©å¤©æ°”\")\n",
        "\n",
        "print(f\"\\nç›¸ä¼¼åº¦:\")\n",
        "print(f\"'äººå·¥æ™ºèƒ½' vs 'AIæŠ€æœ¯': {cosine_similarity(vec1, vec2):.4f}\")\n",
        "print(f\"'äººå·¥æ™ºèƒ½' vs 'ä»Šå¤©å¤©æ°”': {cosine_similarity(vec1, vec3):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC75wGFqaP8O",
        "outputId": "5ed1ad84-783d-4389-864a-2cd15283ed29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ\n",
            "\n",
            "æŸ¥è¯¢: haha\n",
            "æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£:\n",
            "\n",
            "1. hhhhh\n",
            "   æ¥æº: doc7\n",
            "\n",
            "2. ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå¤–å‡º\n",
            "   æ¥æº: doc5\n",
            "\n",
            "3. äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„è¶‹åŠ¿\n",
            "   æ¥æº: doc6\n",
            "\n",
            "å¸¦åˆ†æ•°çš„ç»“æœ:\n",
            "åˆ†æ•° 0.4867: hhhhh\n",
            "åˆ†æ•° 1.0897: ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå¤–å‡º\n",
            "åˆ†æ•° 1.1813: äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„è¶‹åŠ¿\n",
            "\n",
            "âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜\n",
            "âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "FAISS å‘é‡æ•°æ®åº“ç¤ºä¾‹\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# åˆå§‹åŒ– Embedding\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n",
        "\n",
        "# å‡†å¤‡æ–‡æ¡£\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain æ˜¯ä¸€ä¸ª AI åº”ç”¨å¼€å‘æ¡†æ¶\", metadata={\"source\": \"doc1\"}),\n",
        "    Document(page_content=\"Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€\", metadata={\"source\": \"doc2\"}),\n",
        "    Document(page_content=\"æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®\", metadata={\"source\": \"doc3\"}),\n",
        "    Document(page_content=\"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ\", metadata={\"source\": \"doc4\"}),\n",
        "    Document(page_content=\"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå¤–å‡º\", metadata={\"source\": \"doc5\"}),\n",
        "    Document(page_content=\"äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„è¶‹åŠ¿\", metadata={\"source\": \"doc6\"}),\n",
        "    Document(page_content=\"hhhhh\", metadata={\"source\": \"doc7\"}),\n",
        "    Document(page_content=\"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†\", metadata={\"source\": \"doc8\"}),\n",
        "]\n",
        "\n",
        "# åˆ›å»ºå‘é‡æ•°æ®åº“\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ\")\n",
        "\n",
        "# ç›¸ä¼¼åº¦æœç´¢\n",
        "query = \"haha\"\n",
        "results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\næŸ¥è¯¢: {query}\")\n",
        "print(\"æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£:\\n\")\n",
        "\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"{i}. {doc.page_content}\")\n",
        "    print(f\"   æ¥æº: {doc.metadata['source']}\\n\")\n",
        "\n",
        "# å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
        "\n",
        "print(\"å¸¦åˆ†æ•°çš„ç»“æœ:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"åˆ†æ•° {score:.4f}: {doc.page_content}\")\n",
        "\n",
        "# ä¿å­˜å’ŒåŠ è½½\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "print(\"\\nâœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜\")\n",
        "\n",
        "# åŠ è½½\n",
        "loaded_vectorstore = FAISS.load_local(\n",
        "    \"faiss_index\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "print(\"âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDIHYgVza738"
      },
      "source": [
        "# æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi4WwgzGa-vf",
        "outputId": "8b07e60e-b2c1-49ef-c7cf-4e5261f41f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "é—®é¢˜: ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ\n",
            "ç­”æ¡ˆ: æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œæ–‡æ¡£1ä¸­æåˆ°â€œä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå¤–å‡ºâ€ï¼Œå› æ­¤ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "å®Œæ•´çš„ RAG ç³»ç»Ÿ\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. å‡†å¤‡å‘é‡æ•°æ®åº“ï¼ˆå‡è®¾å·²æœ‰ï¼‰\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n",
        "\n",
        "# åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"faiss_index\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "# 2. åˆ›å»ºæ£€ç´¢å™¨\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}  # è¿”å›å‰ 3 ä¸ªæœ€ç›¸å…³æ–‡æ¡£\n",
        ")\n",
        "\n",
        "# 3. åˆ›å»ºæç¤ºè¯æ¨¡æ¿\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\n",
        "\n",
        "ä¸Šä¸‹æ–‡ï¼š\n",
        "{context}\n",
        "\n",
        "é—®é¢˜ï¼š{question}\n",
        "\n",
        "è¦æ±‚ï¼š\n",
        "1. ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯\n",
        "2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·\n",
        "3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´\n",
        "\n",
        "ç­”æ¡ˆï¼š\"\"\")\n",
        "\n",
        "# 4. åˆ›å»ºæ¨¡å‹\n",
        "model = init_chat_model(\n",
        "    \"deepseek-chat\",\n",
        "    model_provider=\"openai\",\n",
        "    base_url=\"https://api.deepseek.com\",\n",
        "    api_key= API_KEY,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# 5. æ„å»º RAG Chain\n",
        "def format_docs(docs):\n",
        "    \"\"\"æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£\"\"\"\n",
        "    return \"\\n\\n\".join([f\"æ–‡æ¡£{i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6. ä½¿ç”¨\n",
        "question = \"ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ\"\n",
        "answer = rag_chain.invoke(question)\n",
        "\n",
        "print(f\"é—®é¢˜: {question}\")\n",
        "print(f\"ç­”æ¡ˆ: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipN7L6Q9bfSZ"
      },
      "source": [
        "# å®æˆ˜é¡¹ç›® - ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\n",
        "ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\n",
        "\n",
        "\n",
        "```\n",
        "â”œâ”€â”€ æ–‡æ¡£å¤„ç†æ¨¡å—\n",
        "â”‚   â”œâ”€â”€ æ–‡æ¡£åŠ è½½\n",
        "â”‚   â”œâ”€â”€ æ–‡æœ¬åˆ†å—\n",
        "â”‚   â””â”€â”€ å‘é‡åŒ–\n",
        "â”œâ”€â”€ æ£€ç´¢æ¨¡å—\n",
        "â”‚   â”œâ”€â”€ å‘é‡æ•°æ®åº“\n",
        "â”‚   â”œâ”€â”€ ç›¸ä¼¼åº¦æœç´¢\n",
        "â”‚   â””â”€â”€ ç»“æœé‡æ’åº\n",
        "â”œâ”€â”€ é—®ç­”æ¨¡å—\n",
        "â”‚   â”œâ”€â”€ æç¤ºè¯ç”Ÿæˆ\n",
        "â”‚   â”œâ”€â”€ LLM è°ƒç”¨\n",
        "â”‚   â””â”€â”€ ç­”æ¡ˆç”Ÿæˆ\n",
        "â””â”€â”€ ç®¡ç†æ¨¡å—\n",
        "    â”œâ”€â”€ æ–‡æ¡£ç®¡ç†\n",
        "    â”œâ”€â”€ æ•°æ®åº“ç»´æŠ¤\n",
        "    â””â”€â”€ æŸ¥è¯¢æ—¥å¿—\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu6JC61tbYVX",
        "outputId": "30192472-be9d-475c-d6f6-4f9585c1dc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ“š ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼ˆLangChain 1.0ï¼‰\n",
            "======================================================================\n",
            "ğŸ”„ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...\n",
            "âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ\n",
            "\n",
            "å‘ç°å·²æœ‰ç´¢å¼•\n",
            "æ˜¯å¦ä½¿ç”¨å·²æœ‰ç´¢å¼•ï¼Ÿ(y/n): y\n",
            "ğŸ”„ æ­£åœ¨åŠ è½½ç´¢å¼•...\n",
            "âœ… ç´¢å¼•åŠ è½½æˆåŠŸ\n",
            "\n",
            "======================================================================\n",
            "ğŸ“ ç¤ºä¾‹æŸ¥è¯¢\n",
            "======================================================================\n",
            "\n",
            "â“ å…¬å¸ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ\n",
            "ğŸ’¡ æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå…¬å¸ä¸»è¦äº§å“å¦‚ä¸‹ï¼š\n",
            "\n",
            "1. **AI å¼€å‘å¹³å°**\n",
            "2. **æ™ºèƒ½å®¢æœç³»ç»Ÿ**ï¼ˆäº§å“åç§°ä¸º SmartCSï¼‰\n",
            "3. **çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ**\n",
            "\n",
            "è¿™äº›ä¿¡æ¯æ¥æºäºã€æ–‡æ¡£ 1ã€‘çš„å…¬å¸ç®€ä»‹éƒ¨åˆ†ã€‚\n",
            "ğŸ“„ å‚è€ƒæ–‡æ¡£: knowledge_docs/company_intro.txt, knowledge_docs/customer_service.txt, knowledge_docs/hr_benefits.txt\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ AI å¼€å‘å¹³å°çš„å®šä»·æ˜¯å¤šå°‘ï¼Ÿ\n",
            "ğŸ’¡ æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼ŒAI å¼€å‘å¹³å°ï¼ˆCloudAI Platformï¼‰çš„å®šä»·æ–¹æ¡ˆå¦‚ä¸‹ï¼š\n",
            "\n",
            "- **å…è´¹ç‰ˆ**ï¼šæ¯æœˆæä¾› 1000 æ¬¡ API è°ƒç”¨ã€‚\n",
            "- **ä¸“ä¸šç‰ˆ**ï¼šæ¯æœˆ Â¥999ï¼Œæä¾› 10 ä¸‡æ¬¡ API è°ƒç”¨ã€‚\n",
            "- **ä¼ä¸šç‰ˆ**ï¼šä¸ºå®šåˆ¶åŒ–æ–¹æ¡ˆï¼Œéœ€è”ç³»é”€å”®è·å–å…·ä½“ä¿¡æ¯ã€‚\n",
            "\n",
            "ä»¥ä¸Šä¿¡æ¯ç»¼åˆè‡ªæ–‡æ¡£ 1ï¼ˆäº§å“ä»‹ç»ï¼‰å’Œæ–‡æ¡£ 3ï¼ˆå¸¸è§é—®é¢˜ï¼‰ã€‚\n",
            "ğŸ“„ å‚è€ƒæ–‡æ¡£: knowledge_docs/product_ai_platform.txt, knowledge_docs/company_intro.txt, knowledge_docs/faq.txt\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ å‘˜å·¥æœ‰å“ªäº›ç¦åˆ©ï¼Ÿ\n",
            "ğŸ’¡ æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå‘˜å·¥ç¦åˆ©ä¿¡æ¯ä¸»è¦æ¥æºäºã€æ–‡æ¡£ 1ã€‘ã€‚å…·ä½“å¦‚ä¸‹ï¼š\n",
            "\n",
            "**è–ªé…¬ç¦åˆ©ï¼š**\n",
            "1. å…·æœ‰ç«äº‰åŠ›çš„è–ªèµ„\n",
            "2. å¹´ç»ˆå¥–é‡‘ï¼ˆ13-16è–ªï¼‰\n",
            "3. è‚¡ç¥¨æœŸæƒ\n",
            "\n",
            "**ä¿é™©ç¦åˆ©ï¼š**\n",
            "1. äº”é™©ä¸€é‡‘\n",
            "2. è¡¥å……å•†ä¸šä¿é™©\n",
            "3. å¹´åº¦ä½“æ£€\n",
            "\n",
            "**å‡æœŸç¦åˆ©ï¼š**\n",
            "1. æ³•å®šèŠ‚å‡æ—¥\n",
            "2. å¸¦è–ªå¹´å‡ï¼ˆ10-20å¤©ï¼‰\n",
            "3. ç—…å‡ã€å©šå‡ã€äº§å‡ç­‰\n",
            "\n",
            "**å…¶ä»–ç¦åˆ©ï¼š**\n",
            "1. ä¸‹åˆèŒ¶\n",
            "2. å¥èº«æˆ¿\n",
            "3. å‘˜å·¥æ—…æ¸¸\n",
            "4. èŠ‚æ—¥ç¤¼å“\n",
            "5. ç”Ÿæ—¥æ´¾å¯¹\n",
            "\n",
            "æ³¨ï¼šã€æ–‡æ¡£ 2ã€‘å’Œã€æ–‡æ¡£ 3ã€‘æœªæåŠå‘˜å·¥ç¦åˆ©ç›¸å…³ä¿¡æ¯ã€‚\n",
            "ğŸ“„ å‚è€ƒæ–‡æ¡£: knowledge_docs/hr_benefits.txt, knowledge_docs/company_intro.txt, knowledge_docs/customer_service.txt\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ\n",
            "ğŸ’¡ æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æŠ€æœ¯æ”¯æŒï¼š\n",
            "\n",
            "**è”ç³»æ–¹å¼ï¼š**\n",
            "- **å‘é€é‚®ä»¶**è‡³ï¼šsupport@company.com\n",
            "- **æ‹¨æ‰“çƒ­çº¿ç”µè¯**ï¼š400-123-4567\n",
            "\n",
            "**ä¿¡æ¯æ¥æºï¼š** ä»¥ä¸Šä¿¡æ¯æ¥æºäºæ–‡æ¡£1ï¼ˆå¸¸è§é—®é¢˜ï¼‰ä¸­çš„Q1éƒ¨åˆ†ã€‚\n",
            "\n",
            "å¦‚æœä»¥ä¸Šæ–¹å¼æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œæ‚¨ä¹Ÿå¯ä»¥è€ƒè™‘é€šè¿‡å…¶ä»–æ¸ é“è”ç³»ï¼Œä¾‹å¦‚æ–‡æ¡£1ä¸­æåˆ°çš„â€œè”ç³»é”€å”®â€å’¨è¯¢ä¼ä¸šç‰ˆç§æœ‰åŒ–éƒ¨ç½²ç­‰äº‹å®œã€‚\n",
            "ğŸ“„ å‚è€ƒæ–‡æ¡£: knowledge_docs/faq.txt, knowledge_docs/company_intro.txt, knowledge_docs/customer_service.txt\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "ğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'export' å¯¼å‡ºæ—¥å¿—ï¼‰\n",
            "======================================================================\n",
            "\n",
            "â“ è¯·è¾“å…¥é—®é¢˜: export\n",
            "âœ… æŸ¥è¯¢æ—¥å¿—å·²å¯¼å‡ºåˆ° query_log.json\n",
            "\n",
            "â“ è¯·è¾“å…¥é—®é¢˜: quit\n",
            "âœ… æŸ¥è¯¢æ—¥å¿—å·²å¯¼å‡ºåˆ° query_log.json\n",
            "å†è§ï¼ğŸ‘‹\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "import json\n",
        "from datetime import datetime\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        docs_directory: str = \"./knowledge_docs\",\n",
        "        index_directory: str = \"./knowledge_index\",\n",
        "        embedding_model: str = \"BAAI/bge-m3\"\n",
        "    ):\n",
        "        self.docs_directory = docs_directory\n",
        "        self.index_directory = index_directory\n",
        "\n",
        "        # åˆå§‹åŒ– Embedding\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n",
        "        print(\"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
        "\n",
        "        # æ–‡æœ¬åˆ†å—å™¨\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # å‘é‡æ•°æ®åº“\n",
        "        self.vectorstore = None\n",
        "\n",
        "        # LLM\n",
        "        self.model = init_chat_model(\n",
        "            \"deepseek-chat\", # Corrected typo from deepseek-caht to deepseek-chat\n",
        "            model_provider=\"openai\",\n",
        "            base_url=\"https://api.deepseek.com\",\n",
        "            api_key= API_KEY,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        # æŸ¥è¯¢å†å²\n",
        "        self.query_log = []\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"æ„å»ºç´¢å¼•\"\"\"\n",
        "        print(\"\\nğŸ“š å¼€å§‹æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...\")\n",
        "\n",
        "        # 1. åŠ è½½æ–‡æ¡£\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½æ–‡æ¡£...\")\n",
        "        loader = DirectoryLoader(\n",
        "            self.docs_directory,\n",
        "            glob=\"**/*.txt\",\n",
        "            loader_cls=TextLoader,\n",
        "            loader_kwargs={\"encoding\": \"utf-8\"},\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            docs = loader.load()\n",
        "            print(f\"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}\")\n",
        "            return False\n",
        "\n",
        "        if not docs:\n",
        "            print(\"âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ç›®å½•ä¸­æœ‰ .txt æ–‡ä»¶\")\n",
        "            return False\n",
        "\n",
        "        # 2. æ–‡æœ¬åˆ†å—\n",
        "        print(\"ğŸ”„ æ­£åœ¨åˆ†å—...\")\n",
        "        split_docs = self.text_splitter.split_documents(docs)\n",
        "        print(f\"âœ… åˆ†å—å®Œæˆï¼Œå…± {len(split_docs)} ä¸ªæ–‡æœ¬å—\")\n",
        "\n",
        "        # 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
        "        print(\"ğŸ”„ æ­£åœ¨å‘é‡åŒ–å¹¶å»ºç«‹ç´¢å¼•...\")\n",
        "        self.vectorstore = FAISS.from_documents(split_docs, self.embeddings)\n",
        "        print(\"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ\")\n",
        "\n",
        "        # 4. ä¿å­˜ç´¢å¼•\n",
        "        os.makedirs(self.index_directory, exist_ok=True)\n",
        "        self.vectorstore.save_local(self.index_directory)\n",
        "        print(f\"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {self.index_directory}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_index(self):\n",
        "        \"\"\"åŠ è½½å·²æœ‰ç´¢å¼•\"\"\"\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½ç´¢å¼•...\")\n",
        "        try:\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                self.index_directory,\n",
        "                self.embeddings,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            print(\"âœ… ç´¢å¼•åŠ è½½æˆåŠŸ\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}\")\n",
        "            return False\n",
        "\n",
        "    def query(self, question: str, k: int = 3) -> dict:\n",
        "        \"\"\"æŸ¥è¯¢çŸ¥è¯†åº“\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"çŸ¥è¯†åº“æœªåˆå§‹åŒ–\"}\n",
        "\n",
        "        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
        "        retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": k}\n",
        "        )\n",
        "\n",
        "        docs = retriever.invoke(question)\n",
        "\n",
        "        # 2. æ„å»ºæç¤ºè¯\n",
        "        prompt = ChatPromptTemplate.from_template(\"\"\"ä½ æ˜¯ä¼ä¸šçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚\n",
        "\n",
        "          è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚\n",
        "\n",
        "          æ–‡æ¡£ç‰‡æ®µï¼š\n",
        "          {context}\n",
        "\n",
        "          ç”¨æˆ·é—®é¢˜ï¼š{question}\n",
        "\n",
        "          å›ç­”è¦æ±‚ï¼š\n",
        "          1. ä»…åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”\n",
        "          2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·\n",
        "          3. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æ˜“æ‡‚\n",
        "          4. å¦‚æœç­”æ¡ˆæ¥è‡ªå¤šä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œè¯·ç»¼åˆå›ç­”\n",
        "\n",
        "          ç­”æ¡ˆï¼š\"\"\")\n",
        "\n",
        "        # 3. æ ¼å¼åŒ–æ–‡æ¡£\n",
        "        def format_docs(docs):\n",
        "            return \"\\n\\n---\\n\\n\".join([\n",
        "                f\"ã€æ–‡æ¡£ {i+1}ã€‘\\næ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\\nå†…å®¹: {doc.page_content}\"\n",
        "                for i, doc in enumerate(docs)\n",
        "            ])\n",
        "\n",
        "        # 4. æ„å»º RAG Chain\n",
        "        rag_chain = (\n",
        "            {\"context\": lambda x: format_docs(docs), \"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | self.model\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        # 5. ç”Ÿæˆç­”æ¡ˆ\n",
        "        answer = rag_chain.invoke(question)\n",
        "\n",
        "        # 6. è®°å½•æŸ¥è¯¢\n",
        "        self.query_log.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [doc.metadata.get('source', 'æœªçŸ¥') for doc in docs],\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": docs,\n",
        "            \"source_count\": len(docs)\n",
        "        }\n",
        "\n",
        "    def export_logs(self, filename: str = \"query_log.json\"):\n",
        "        \"\"\"å¯¼å‡ºæŸ¥è¯¢æ—¥å¿—\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.query_log, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"âœ… æŸ¥è¯¢æ—¥å¿—å·²å¯¼å‡ºåˆ° {filename}\")\n",
        "\n",
        "def create_sample_docs(directory: str = \"./knowledge_docs\"):\n",
        "    \"\"\"åˆ›å»ºç¤ºä¾‹æ–‡æ¡£\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    docs = {\n",
        "        \"company_intro.txt\": \"\"\"å…¬å¸ç®€ä»‹\\n\\n        æˆ‘ä»¬æ˜¯ä¸€å®¶ä¸“æ³¨äºäººå·¥æ™ºèƒ½æŠ€æœ¯ç ”å‘çš„ç§‘æŠ€å…¬å¸ï¼Œæˆç«‹äº2020å¹´ã€‚\\n\\n        å…¬å¸ä½¿å‘½ï¼šè®©AIæŠ€æœ¯æ™®æƒ æ¯ä¸ªäººã€‚\\n\\n        ä¸»è¦äº§å“ï¼š\\n        1. AI å¼€å‘å¹³å°\\n        2. æ™ºèƒ½å®¢æœç³»ç»Ÿ\\n        3. çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\\n\\n        å…¬å¸è§„æ¨¡ï¼šå‘˜å·¥200äººï¼Œå…¶ä¸­ç ”å‘äººå‘˜å 70%ã€‚\\n\\n        å…¬å¸åœ°å€ï¼šåŒ—äº¬å¸‚æµ·æ·€åŒºä¸­å…³æ‘è½¯ä»¶å›­ã€‚\"\"\",\n",
        "\n",
        "                \"product_ai_platform.txt\": \"\"\"AI å¼€å‘å¹³å°\\n\\n        äº§å“åç§°ï¼šCloudAI Platform\\n\\n        åŠŸèƒ½ç‰¹æ€§ï¼š\\n        1. æ¨¡å‹è®­ç»ƒï¼šæ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ æ¡†æ¶\\n        2. æ¨¡å‹éƒ¨ç½²ï¼šä¸€é”®éƒ¨ç½²åˆ°äº‘ç«¯\\n        3. API æ¥å£ï¼šæä¾› RESTful API\\n        4. ç›‘æ§å‘Šè­¦ï¼šå®æ—¶ç›‘æ§æ¨¡å‹æ€§èƒ½\\n\\n        æŠ€æœ¯ä¼˜åŠ¿ï¼š\\n        - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ\\n        - è‡ªåŠ¨è°ƒå‚\\n        - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†\\n        - A/B æµ‹è¯•\\n\\n        å®šä»·æ–¹æ¡ˆï¼š\\n        - å…è´¹ç‰ˆï¼šæ¯æœˆ 1000 æ¬¡ API è°ƒç”¨\\n        - ä¸“ä¸šç‰ˆï¼šæ¯æœˆ Â¥999ï¼Œ10ä¸‡æ¬¡è°ƒç”¨\\n        - ä¼ä¸šç‰ˆï¼šå®šåˆ¶åŒ–ï¼Œè”ç³»é”€å”®\"\"\",\n",
        "\n",
        "                \"customer_service.txt\": \"\"\"æ™ºèƒ½å®¢æœç³»ç»Ÿ\\n\\n        äº§å“åç§°ï¼šSmartCS\\n\\n        æ ¸å¿ƒåŠŸèƒ½ï¼š\\n        1. å¤šè½®å¯¹è¯ï¼šæ”¯æŒå¤æ‚çš„å¤šè½®å¯¹è¯\\n        2. çŸ¥è¯†åº“ï¼šé›†æˆä¼ä¸šçŸ¥è¯†åº“\\n        3. å·¥å•ç³»ç»Ÿï¼šè‡ªåŠ¨åˆ›å»ºå·¥å•\\n        4. æ•°æ®åˆ†æï¼šå®¢æœæ•°æ®å¯è§†åŒ–\\n\n",
        "        é€‚ç”¨åœºæ™¯ï¼š\\n        - ç”µå•†å®¢æœ\\n        - é“¶è¡Œå®¢æœ\\n        - æ”¿åºœæœåŠ¡çƒ­çº¿\\n\\n        æ¥å…¥æ–¹å¼ï¼š\\n        - ç½‘é¡µæ’ä»¶\\n        - ç§»ä¿¡å…¬ä¼—å·\\n        - å°ç¨‹åº\\n        - APP SDK\\n\\n        å®¢æˆ·æ¡ˆä¾‹ï¼š\\n        æŸç”µå•†å¹³å°æ¥å…¥åï¼Œå®¢æœæ•ˆç‡æå‡ 60%ï¼Œå®¢æˆ·æ»¡æ„åº¦æé«˜ 35%ã€‚\"\"\",\n",
        "\n",
        "                \"hr_benefits.txt\": \"\"\"å‘˜å·¥ç¦åˆ©\\n\\n        æˆ‘å¸ä¸ºå‘˜å·¥æä¾›å®Œå–„çš„ç¦åˆ©ä½“ç³»ï¼š\\n\\n        è–ªé…¬ç¦åˆ©ï¼š\\n        1. å…·æœ‰ç«äº‰åŠ›çš„è–ªèµ„\\n        2. å¹´ç»ˆå¥–é‡‘ï¼ˆ13-16è–ªï¼‰\\n        3. è‚¡ç¥¨æœŸæƒ\\n\\n        ä¿é™©ç¦åˆ©ï¼š\\n        1. äº”é™©ä¸€é‡‘\\n        2. è¡¥å……å•†ä¸šä¿é™©\\n        3. å¹´åº¦ä½“æ£€\\n\\n        å‡æœŸç¦åˆ©ï¼š\\n        1. æ³•å®šèŠ‚å‡æ—¥\\n        2. å¸¦è–ªå¹´å‡ï¼ˆ10-20å¤©ï¼‰\\n        3. ç—…å‡ã€å©šå‡ã€äº§å‡ç­‰\\n\\n        å…¶ä»–ç¦åˆ©ï¼š\\n        1. ä¸‹åˆèŒ¶\\n        2. å¥èº«æˆ¿\\n        3. å‘˜å·¥æ—…æ¸¸\\n        4. èŠ‚æ—¥ç¤¼å“\\n        5. ç”Ÿæ—¥æ´¾å¯¹\"\"\",\n",
        "\n",
        "                \"faq.txt\": \"\"\"å¸¸è§é—®é¢˜\\n\\n        Q1: å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ\\n        A1: å‘é€é‚®ä»¶åˆ° support@company.comï¼Œæˆ–æ‹¨æ‰“çƒ­çº¿ 400-123-4567ã€‚\\n\\n        Q2: API è°ƒç”¨é™åˆ¶æ˜¯å¤šå°‘ï¼Ÿ\\n        A2: å…è´¹ç‰ˆæ¯æœˆ1000æ¬¡ï¼Œä¸“ä¸šç‰ˆ10ä¸‡æ¬¡ï¼Œä¼ä¸šç‰ˆå¯å®šåˆ¶ã€‚\\n\\n        Q3: æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ\\n        A3: æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€å¯¹å…¬è½¬è´¦ã€‚\\n\\n        Q4: æ•°æ®å®‰å…¨å¦‚ä½•ä¿éšœï¼Ÿ\\n        A4: é‡‡ç”¨é“¶è¡Œçº§åŠ å¯†ï¼Œé€šè¿‡ ISO27001 è®¤è¯ï¼Œæ•°æ®ä¸­å¿ƒè®¾åœ¨å›½å†…ã€‚\\n\\n        Q5: æ˜¯å¦æ”¯æŒç§æœ‰åŒ–éƒ¨ç½²ï¼Ÿ\\n        A5: ä¼ä¸šç‰ˆæ”¯æŒç§æœ‰åŒ–éƒ¨ç½²ï¼Œå…·ä½“æ–¹æ¡ˆè¯·è”ç³»é”€å”®ã€‚\"\"\"\n",
        "    }\n",
        "\n",
        "    for filename, content in docs.items():\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"âœ… å·²åˆ›å»º {len(docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£åˆ° {directory}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ğŸ“š ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼ˆLangChain 1.0ï¼‰\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    kb = KnowledgeBase()\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„ç´¢å¼•\n",
        "    if os.path.exists(kb.index_directory):\n",
        "        print(\"\\nå‘ç°å·²æœ‰ç´¢å¼•\")\n",
        "        choice = input(\"æ˜¯å¦ä½¿ç”¨å·²æœ‰ç´¢å¼•ï¼Ÿ(y/n): \").strip().lower()\n",
        "        if choice == 'y':\n",
        "            if not kb.load_index():\n",
        "                print(\"åŠ è½½å¤±è´¥ï¼Œéœ€è¦é‡æ–°æ„å»º\")\n",
        "                create_sample_docs()\n",
        "                kb.build_index()\n",
        "        else:\n",
        "            create_sample_docs()\n",
        "            kb.build_index()\n",
        "    else:\n",
        "        print(\"\\næœªæ‰¾åˆ°ç´¢å¼•ï¼Œå¼€å§‹æ„å»º...\")\n",
        "        create_sample_docs()\n",
        "        if not kb.build_index():\n",
        "            print(\"âŒ æ„å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º\")\n",
        "            return\n",
        "\n",
        "    # ç¤ºä¾‹æŸ¥è¯¢\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ“ ç¤ºä¾‹æŸ¥è¯¢\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_questions = [\n",
        "        \"å…¬å¸ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ\",\n",
        "        \"AI å¼€å‘å¹³å°çš„å®šä»·æ˜¯å¤šå°‘ï¼Ÿ\",\n",
        "        \"å‘˜å·¥æœ‰å“ªäº›ç¦åˆ©ï¼Ÿ\",\n",
        "        \"å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nâ“ {question}\")\n",
        "        result = kb.query(question)\n",
        "\n",
        "        print(f\"ğŸ’¡ {result['answer']}\")\n",
        "        print(f\"ğŸ“„ å‚è€ƒæ–‡æ¡£: {', '.join([doc.metadata['source'] for doc in result['sources']])}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # äº¤äº’æ¨¡å¼\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'export' å¯¼å‡ºæ—¥å¿—ï¼‰\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nâ“ è¯·è¾“å…¥é—®é¢˜: \").strip()\n",
        "\n",
        "        if question.lower() == 'quit':\n",
        "            kb.export_logs()\n",
        "            print(\"å†è§ï¼ğŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if question.lower() == 'export':\n",
        "            kb.export_logs()\n",
        "            continue\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        result = kb.query(question)\n",
        "\n",
        "        print(f\"\\nğŸ’¡ ç­”æ¡ˆ:\\n{result['answer']}\")\n",
        "        print(f\"\\nğŸ“š å‚è€ƒæ¥æº:\")\n",
        "        for i, doc in enumerate(result['sources'], 1):\n",
        "            print(f\"  {i}. {doc.metadata['source']}\")\n",
        "            print(f\"     å†…å®¹ç‰‡æ®µ: {doc.page_content[:100]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Re4taex_Wyd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "å®æˆ˜é¡¹ç›®ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼ˆLangChain 1.0ï¼‰\n",
        "\n",
        "åŠŸèƒ½ï¼š\n",
        "- æ–‡æ¡£æ‰¹é‡å¯¼å…¥\n",
        "- æ™ºèƒ½åˆ†å—\n",
        "- å‘é‡æ£€ç´¢\n",
        "- RAG é—®ç­”\n",
        "- ç­”æ¡ˆæº¯æº\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "import json\n",
        "from datetime import datetime\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        docs_directory: str = \"./knowledge_docs\",\n",
        "        index_directory: str = \"./knowledge_index\",\n",
        "        embedding_model: str = \"BAAI/bge-m3\"\n",
        "    ):\n",
        "        self.docs_directory = docs_directory\n",
        "        self.index_directory = index_directory\n",
        "\n",
        "        # åˆå§‹åŒ– Embedding\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...\")\n",
        "        self.embeddings =  OpenAIEmbeddings(\n",
        "          base_url=\"https://api.siliconflow.cn/v1\",\n",
        "          api_key=API_KEY,\n",
        "          model=\"BAAI/bge-m3\"\n",
        "        )\n",
        "        print(\"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
        "\n",
        "        # æ–‡æœ¬åˆ†å—å™¨\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # å‘é‡æ•°æ®åº“\n",
        "        self.vectorstore = None\n",
        "\n",
        "        # LLM\n",
        "        self.model = init_chat_model(\n",
        "            \"Qwen/Qwen3-8B\",\n",
        "            model_provider=\"openai\",\n",
        "            base_url=\"https://api.siliconflow.cn/v1\",\n",
        "            api_key= API_KEY,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        # æŸ¥è¯¢å†å²\n",
        "        self.query_log = []\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"æ„å»ºç´¢å¼•\"\"\"\n",
        "        print(\"\\nğŸ“š å¼€å§‹æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...\")\n",
        "\n",
        "        # 1. åŠ è½½æ–‡æ¡£\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½æ–‡æ¡£...\")\n",
        "        loader = DirectoryLoader(\n",
        "            self.docs_directory,\n",
        "            glob=\"**/*.txt\",\n",
        "            loader_cls=TextLoader,\n",
        "            loader_kwargs={\"encoding\": \"utf-8\"},\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            docs = loader.load()\n",
        "            print(f\"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}\")\n",
        "            return False\n",
        "\n",
        "        if not docs:\n",
        "            print(\"âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ç›®å½•ä¸­æœ‰ .txt æ–‡ä»¶\")\n",
        "            return False\n",
        "\n",
        "        # 2. æ–‡æœ¬åˆ†å—\n",
        "        print(\"ğŸ”„ æ­£åœ¨åˆ†å—...\")\n",
        "        split_docs = self.text_splitter.split_documents(docs)\n",
        "        print(f\"âœ… åˆ†å—å®Œæˆï¼Œå…± {len(split_docs)} ä¸ªæ–‡æœ¬å—\")\n",
        "\n",
        "        # 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
        "        print(\"ğŸ”„ æ­£åœ¨å‘é‡åŒ–å¹¶å»ºç«‹ç´¢å¼•...\")\n",
        "        self.vectorstore = FAISS.from_documents(split_docs, self.embeddings)\n",
        "        print(\"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ\")\n",
        "\n",
        "        # 4. ä¿å­˜ç´¢å¼•\n",
        "        os.makedirs(self.index_directory, exist_ok=True)\n",
        "        self.vectorstore.save_local(self.index_directory)\n",
        "        print(f\"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {self.index_directory}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_index(self):\n",
        "        \"\"\"åŠ è½½å·²æœ‰ç´¢å¼•\"\"\"\n",
        "        print(\"ğŸ”„ æ­£åœ¨åŠ è½½ç´¢å¼•...\")\n",
        "        try:\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                self.index_directory,\n",
        "                self.embeddings,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            print(\"âœ… ç´¢å¼•åŠ è½½æˆåŠŸ\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}\")\n",
        "            return False\n",
        "\n",
        "    def query(self, question: str, k: int = 3) -> dict:\n",
        "        \"\"\"æŸ¥è¯¢çŸ¥è¯†åº“\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"çŸ¥è¯†åº“æœªåˆå§‹åŒ–\"}\n",
        "\n",
        "        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
        "        retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": k}\n",
        "        )\n",
        "\n",
        "        docs = retriever.invoke(question)\n",
        "\n",
        "        # 2. æ„å»ºæç¤ºè¯\n",
        "        prompt = ChatPromptTemplate.from_template(\"\"\"ä½ æ˜¯ä¼ä¸šçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚\n",
        "\n",
        "          è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚\n",
        "\n",
        "          æ–‡æ¡£ç‰‡æ®µï¼š\n",
        "          {context}\n",
        "\n",
        "          ç”¨æˆ·é—®é¢˜ï¼š{question}\n",
        "\n",
        "          å›ç­”è¦æ±‚ï¼š\n",
        "          1. ä»…åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”\n",
        "          2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·\n",
        "          3. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æ˜“æ‡‚\n",
        "          4. å¦‚æœç­”æ¡ˆæ¥è‡ªå¤šä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œè¯·ç»¼åˆå›ç­”\n",
        "\n",
        "          ç­”æ¡ˆï¼š\"\"\")\n",
        "\n",
        "        # 3. æ ¼å¼åŒ–æ–‡æ¡£\n",
        "        def format_docs(docs):\n",
        "            return \"\\n\\n---\\n\\n\".join([\n",
        "                f\"ã€æ–‡æ¡£ {i+1}ã€‘\\næ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\\nå†…å®¹: {doc.page_content}\"\n",
        "                for i, doc in enumerate(docs)\n",
        "            ])\n",
        "\n",
        "        # 4. æ„å»º RAG Chain\n",
        "        rag_chain = (\n",
        "            {\"context\": lambda x: format_docs(docs), \"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | self.model\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        # 5. ç”Ÿæˆç­”æ¡ˆ\n",
        "        answer = rag_chain.invoke(question)\n",
        "\n",
        "        # 6. è®°å½•æŸ¥è¯¢\n",
        "        self.query_log.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [doc.metadata.get('source', 'æœªçŸ¥') for doc in docs],\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": docs,\n",
        "            \"source_count\": len(docs)\n",
        "        }\n",
        "\n",
        "    def export_logs(self, filename: str = \"query_log.json\"):\n",
        "        \"\"\"å¯¼å‡ºæŸ¥è¯¢æ—¥å¿—\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.query_log, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"âœ… æŸ¥è¯¢æ—¥å¿—å·²å¯¼å‡ºåˆ° {filename}\")\n",
        "\n",
        "def create_sample_docs(directory: str = \"./knowledge_docs\"):\n",
        "    \"\"\"åˆ›å»ºç¤ºä¾‹æ–‡æ¡£\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    docs = {\n",
        "        \"company_intro.txt\": \"\"\"å…¬å¸ç®€ä»‹\n",
        "\n",
        "        æˆ‘ä»¬æ˜¯ä¸€å®¶ä¸“æ³¨äºäººå·¥æ™ºèƒ½æŠ€æœ¯ç ”å‘çš„ç§‘æŠ€å…¬å¸ï¼Œæˆç«‹äº2020å¹´ã€‚\n",
        "\n",
        "        å…¬å¸ä½¿å‘½ï¼šè®©AIæŠ€æœ¯æ™®æƒ æ¯ä¸ªäººã€‚\n",
        "\n",
        "        ä¸»è¦äº§å“ï¼š\n",
        "        1. AI å¼€å‘å¹³å°\n",
        "        2. æ™ºèƒ½å®¢æœç³»ç»Ÿ\n",
        "        3. çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\n",
        "\n",
        "        å…¬å¸è§„æ¨¡ï¼šå‘˜å·¥200äººï¼Œå…¶ä¸­ç ”å‘äººå‘˜å 70%ã€‚\n",
        "\n",
        "        å…¬å¸åœ°å€ï¼šåŒ—äº¬å¸‚æµ·æ·€åŒºä¸­å…³æ‘è½¯ä»¶å›­ã€‚\"\"\",\n",
        "\n",
        "                \"product_ai_platform.txt\": \"\"\"AI å¼€å‘å¹³å°\n",
        "\n",
        "        äº§å“åç§°ï¼šCloudAI Platform\n",
        "\n",
        "        åŠŸèƒ½ç‰¹æ€§ï¼š\n",
        "        1. æ¨¡å‹è®­ç»ƒï¼šæ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "        2. æ¨¡å‹éƒ¨ç½²ï¼šä¸€é”®éƒ¨ç½²åˆ°äº‘ç«¯\n",
        "        3. API æ¥å£ï¼šæä¾› RESTful API\n",
        "        4. ç›‘æ§å‘Šè­¦ï¼šå®æ—¶ç›‘æ§æ¨¡å‹æ€§èƒ½\n",
        "\n",
        "        æŠ€æœ¯ä¼˜åŠ¿ï¼š\n",
        "        - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ\n",
        "        - è‡ªåŠ¨è°ƒå‚\n",
        "        - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†\n",
        "        - A/B æµ‹è¯•\n",
        "\n",
        "        å®šä»·æ–¹æ¡ˆï¼š\n",
        "        - å…è´¹ç‰ˆï¼šæ¯æœˆ 1000 æ¬¡ API è°ƒç”¨\n",
        "        - ä¸“ä¸šç‰ˆï¼šæ¯æœˆ Â¥999ï¼Œ10ä¸‡æ¬¡è°ƒç”¨\n",
        "        - ä¼ä¸šç‰ˆï¼šå®šåˆ¶åŒ–ï¼Œè”ç³»é”€å”®\"\"\",\n",
        "\n",
        "                \"customer_service.txt\": \"\"\"æ™ºèƒ½å®¢æœç³»ç»Ÿ\n",
        "\n",
        "        äº§å“åç§°ï¼šSmartCS\n",
        "\n",
        "        æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
        "        1. å¤šè½®å¯¹è¯ï¼šæ”¯æŒå¤æ‚çš„å¤šè½®å¯¹è¯\n",
        "        2. çŸ¥è¯†åº“ï¼šé›†æˆä¼ä¸šçŸ¥è¯†åº“\n",
        "        3. å·¥å•ç³»ç»Ÿï¼šè‡ªåŠ¨åˆ›å»ºå·¥å•\n",
        "        4. æ•°æ®åˆ†æï¼šå®¢æœæ•°æ®å¯è§†åŒ–\n",
        "\n",
        "        é€‚ç”¨åœºæ™¯ï¼š\n",
        "        - ç”µå•†å®¢æœ\n",
        "        - é“¶è¡Œå®¢æœ\n",
        "        - æ”¿åºœæœåŠ¡çƒ­çº¿\n",
        "\n",
        "        æ¥å…¥æ–¹å¼ï¼š\n",
        "        - ç½‘é¡µæ’ä»¶\n",
        "        - ç§»ä¿¡å…¬ä¼—å·\n",
        "        - å°ç¨‹åº\n",
        "        - APP SDK\n",
        "\n",
        "        å®¢æˆ·æ¡ˆä¾‹ï¼š\n",
        "        æŸç”µå•†å¹³å°æ¥å…¥åï¼Œå®¢æœæ•ˆç‡æå‡ 60%ï¼Œå®¢æˆ·æ»¡æ„åº¦æé«˜ 35%ã€‚\"\"\",\n",
        "\n",
        "                \"hr_benefits.txt\": \"\"\"å‘˜å·¥ç¦åˆ©\n",
        "\n",
        "        æˆ‘å¸ä¸ºå‘˜å·¥æä¾›å®Œå–„çš„ç¦åˆ©ä½“ç³»ï¼š\n",
        "\n",
        "        è–ªé…¬ç¦åˆ©ï¼š\n",
        "        1. å…·æœ‰ç«äº‰åŠ›çš„è–ªèµ„\n",
        "        2. å¹´ç»ˆå¥–é‡‘ï¼ˆ13-16è–ªï¼‰\n",
        "        3. è‚¡ç¥¨æœŸæƒ\n",
        "\n",
        "        ä¿é™©ç¦åˆ©ï¼š\n",
        "        1. äº”é™©ä¸€é‡‘\n",
        "        2. è¡¥å……å•†ä¸šä¿é™©\n",
        "        3. å¹´åº¦ä½“æ£€\n",
        "\n",
        "        å‡æœŸç¦åˆ©ï¼š\n",
        "        1. æ³•å®šèŠ‚å‡æ—¥\n",
        "        2. å¸¦è–ªå¹´å‡ï¼ˆ10-20å¤©ï¼‰\n",
        "        3. ç—…å‡ã€å©šå‡ã€äº§å‡ç­‰\n",
        "\n",
        "        å…¶ä»–ç¦åˆ©ï¼š\n",
        "        1. ä¸‹åˆèŒ¶\n",
        "        2. å¥èº«æˆ¿\n",
        "        3. å‘˜å·¥æ—…æ¸¸\n",
        "        4. èŠ‚æ—¥ç¤¼å“\n",
        "        5. ç”Ÿæ—¥æ´¾å¯¹\"\"\",\n",
        "\n",
        "                \"faq.txt\": \"\"\"å¸¸è§é—®é¢˜\n",
        "\n",
        "        Q1: å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ\n",
        "        A1: å‘é€é‚®ä»¶åˆ° support@company.comï¼Œæˆ–æ‹¨æ‰“çƒ­çº¿ 400-123-4567ã€‚\n",
        "\n",
        "        Q2: API è°ƒç”¨é™åˆ¶æ˜¯å¤šå°‘ï¼Ÿ\n",
        "        A2: å…è´¹ç‰ˆæ¯æœˆ1000æ¬¡ï¼Œä¸“ä¸šç‰ˆ10ä¸‡æ¬¡ï¼Œä¼ä¸šç‰ˆå¯å®šåˆ¶ã€‚\n",
        "\n",
        "        Q3: æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ\n",
        "        A3: æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€å¯¹å…¬è½¬è´¦ã€‚\n",
        "\n",
        "        Q4: æ•°æ®å®‰å…¨å¦‚ä½•ä¿éšœï¼Ÿ\n",
        "        A4: é‡‡ç”¨é“¶è¡Œçº§åŠ å¯†ï¼Œé€šè¿‡ ISO27001 è®¤è¯ï¼Œæ•°æ®ä¸­å¿ƒè®¾åœ¨å›½å†…ã€‚\n",
        "\n",
        "        Q5: æ˜¯å¦æ”¯æŒç§æœ‰åŒ–éƒ¨ç½²ï¼Ÿ\n",
        "        A5: ä¼ä¸šç‰ˆæ”¯æŒç§æœ‰åŒ–éƒ¨ç½²ï¼Œå…·ä½“æ–¹æ¡ˆè¯·è”ç³»é”€å”®ã€‚\"\"\"\n",
        "    }\n",
        "\n",
        "    for filename, content in docs.items():\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"âœ… å·²åˆ›å»º {len(docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£åˆ° {directory}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ğŸ“š ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼ˆLangChain 1.0ï¼‰\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    kb = KnowledgeBase()\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„ç´¢å¼•\n",
        "    if os.path.exists(kb.index_directory):\n",
        "        print(\"\\nå‘ç°å·²æœ‰ç´¢å¼•\")\n",
        "        choice = input(\"æ˜¯å¦ä½¿ç”¨å·²æœ‰ç´¢å¼•ï¼Ÿ(y/n): \").strip().lower()\n",
        "        if choice == 'y':\n",
        "            if not kb.load_index():\n",
        "                print(\"åŠ è½½å¤±è´¥ï¼Œéœ€è¦é‡æ–°æ„å»º\")\n",
        "                create_sample_docs()\n",
        "                kb.build_index()\n",
        "        else:\n",
        "            create_sample_docs()\n",
        "            kb.build_index()\n",
        "    else:\n",
        "        print(\"\\næœªæ‰¾åˆ°ç´¢å¼•ï¼Œå¼€å§‹æ„å»º...\")\n",
        "        create_sample_docs()\n",
        "        if not kb.build_index():\n",
        "            print(\"âŒ æ„å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º\")\n",
        "            return\n",
        "\n",
        "    # ç¤ºä¾‹æŸ¥è¯¢\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ“ ç¤ºä¾‹æŸ¥è¯¢\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_questions = [\n",
        "        \"å…¬å¸ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ\",\n",
        "        \"AI å¼€å‘å¹³å°çš„å®šä»·æ˜¯å¤šå°‘ï¼Ÿ\",\n",
        "        \"å‘˜å·¥æœ‰å“ªäº›ç¦åˆ©ï¼Ÿ\",\n",
        "        \"å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nâ“ {question}\")\n",
        "        result = kb.query(question)\n",
        "\n",
        "        print(f\"ğŸ’¡ {result['answer']}\")\n",
        "        print(f\"ğŸ“„ å‚è€ƒæ–‡æ¡£: {', '.join([doc.metadata['source'] for doc in result['sources']])}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # äº¤äº’æ¨¡å¼\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'export' å¯¼å‡ºæ—¥å¿—ï¼‰\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nâ“ è¯·è¾“å…¥é—®é¢˜: \").strip()\n",
        "\n",
        "        if question.lower() == 'quit':\n",
        "            kb.export_logs()\n",
        "            print(\"å†è§ï¼ğŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if question.lower() == 'export':\n",
        "            kb.export_logs()\n",
        "            continue\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        result = kb.query(question)\n",
        "\n",
        "        print(f\"\\nğŸ’¡ ç­”æ¡ˆ:\\n{result['answer']}\")\n",
        "        print(f\"\\nğŸ“š å‚è€ƒæ¥æº:\")\n",
        "        for i, doc in enumerate(result['sources'], 1):\n",
        "            print(f\"  {i}. {doc.metadata['source']}\")\n",
        "            print(f\"     å†…å®¹ç‰‡æ®µ: {doc.page_content[:100]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xplnog8Ne5Be"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}